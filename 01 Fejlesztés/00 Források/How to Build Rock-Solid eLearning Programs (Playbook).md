# How to Build Rock-Solid eLearning Programs (Playbook)

**Assumptions:** The playbook is for **corporate/professional training**. We assume a generic Learning Management System (LMS) with common features (quizzes, modules, discussion forums, etc.) and the involvement of **Subject Matter Experts (SMEs)** for content. Platform-specific details (e.g. Moodle vs. others) are avoided in favor of universal best practices.

## A. Executive One-Pager: “How to Build Rock-Solid eLearning Programs”

*How to use this:* Share this one-pager with stakeholders as a high-level summary of the end-to-end methodology.

**1\. Start with Science & Outcomes – Not Content:** Effective programs begin with **backward design** – define what learners must do **at the end**, then plan assessments and content[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials). Leverage **learning science principles** to maximize retention and transfer: \- Use **spaced practice** (spread learning over time) and **retrieval practice** (frequent self-quizzing) – proven to dramatically improve long-term memory[\[2\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Retrieval%20practice%20is%20essentially%20the,you%20learn%20and%20remember%20it)[\[3\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Spaced%20practice). \- Manage **cognitive load:** break complex topics into bite-sized chunks, and remove extraneous info. Combine words and visuals (**dual coding**) but avoid irrelevant media (Mayer’s **coherence principle**)[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2). This reduces overload and boosts understanding. \- **Desirable difficulties:** Introduce challenges like **interleaving** (mixing topics) – it may feel harder but yields higher retention (e.g. 61% vs 38% scores after one month)[\[5\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20a%20recent%20study%2C%20nearly,term%20mathematics%20learning). Effortful learning (solving problems, self-explanation) leads to deeper mastery[\[6\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=Researchers%20refer%20to%20the%20benefits,and%20practice%20what%20they%20know). \- Embrace **feedback & mastery:** Provide immediate, specific feedback and allow retries. Aim for mastery learning where **each unit’s objectives are mastered by all** via corrective loops[\[7\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=Mastery%20learning%E2%80%99s%20goal%20is%20for,accomplish%20the%20predefined%20level%20of)[\[8\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=mastery%20are%20given%20corrective%20exercises,remediated%20until%20they%20demonstrate%20mastery).

**2\. Structure Programs for Adults:** Design a clear hierarchy – **Program → Courses → Modules → Lessons** – with measurable outcomes at each level. For busy professionals: \- Make learning **modular and mobile-friendly:** Use 15–30 minute microlearning lessons for flexibility. \- **Blend asynchronous first,** with optional live sessions for practice. Adults appreciate self-paced learning with support available. \- **Scaffold and personalize:** Sequence modules from fundamentals to advanced; provide prerequisites or adaptive paths so learners can skip or get remedial content as needed. Use narrative cases or scenarios to maintain interest. Include a capstone project or portfolio to demonstrate cumulative skills in context. \- Align each outcome to a **competency framework** if applicable (e.g. map course outcomes to professional competencies). This creates a throughline from learning to on-the-job application, and you can collect evidence for each competency (quiz scores, project artifacts).

**3\. Engage Through Lesson Pattern & Media:** Within lessons, follow a consistent flow that primes, teaches, practices, and reinforces: \- **Hook** learners early (a question, story, or problem scenario relevant to their job). State clear **objectives** so they know the WIIFM (“what’s in it for me”). \- **Recall prior knowledge** with a quick activity or question to connect new ideas to what they know. \- **Exposition**: Present content in manageable chunks (short videos, slides, or text). Apply **multimedia best practices** – e.g. use explanatory images with audio narration, not giant text blocks[\[9\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=1)[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2). Ensure **accessibility** (caption videos, use alt text on images, high-contrast design). \- **Practice and interaction:** After each chunk, include a brief **guided practice** (e.g. a sample problem with solution walk-through) followed by **independent practice** (unguided question, exercise, or simulation). This could be a quick quiz (with feedback on each attempt) or a scenario decision. These frequent interactions reset attention and reinforce learning. \- **Reflection and social learning:** Prompt learners to reflect (e.g. “What would you do in your context?”) or discuss in forums/chat. Social polls or peer-sharing of ideas tap into adult learners’ experiences and **motivation** (supporting their need for relatedness and autonomy[\[10\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=1,when%20they%20feel%20supported%20to)[\[11\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=3,it%20motivates%20them%20to%20succeed)). \- **Wrap-up:** End the lesson by summarizing key points and how they tie to real-world outcomes. Encourage learners to plan how to apply the knowledge on the job.

**4\. Assess to Teach (Not Just Test):** Use a mix of **formative assessments** (ungraded quizzes, self-checks) and **summative assessments** (graded tests, projects) aligned to the objectives. Every outcome should have a defined **evidence of achievement** – e.g. a score above X on a quiz or a satisfactory project evaluated with a rubric. Design assessments to be **authentic** when possible: mirror real tasks or decisions the learner will face. Provide clear success criteria (share rubrics upfront) to guide learners. If learners fall short, offer **remediation loops**: targeted feedback, resources, and a chance to try again, embodying a mastery-learning mindset.

**5\. Iterate with Data & Feedback:** Launch pilots and gather feedback. Track learning analytics – e.g. **engagement data** (login frequency, video watch rates), **assessment data** (quiz attempts, item difficulty stats, time on tasks) – to identify where learners struggle or disengage. Use Kirkpatrick’s framework to evaluate success: \- **Level 1 (Reaction):** Collect learner feedback on the training (was it engaging, relevant?). \- **Level 2 (Learning):** Measure improvement (quizzes, skill demos). \- **Level 3 (Behavior):** Look for changes on the job (via supervisor surveys or performance metrics). \- **Level 4 (Results):** Connect training to business outcomes (e.g. sales up, error rates down)[\[12\]](https://xapi.com/kirkpatrick/#:~:text=1%20Reaction%20What%20did%20the,as%20a%20result%20of%20the)[\[13\]](https://xapi.com/kirkpatrick/#:~:text=4%20Results%20What%20was%20the,have%20ready%20access%20to%20it). \- **Level 5 (ROI):** If possible, calculate ROI by comparing benefits (e.g. productivity gains or cost savings) to total program cost[\[14\]](https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/#:~:text=One%20of%20the%20most%20frequently,ROI). Even if hard to pin down, discussing ROI focuses design on business value.

Use these data to **continuously improve** content (update or clarify lessons where quiz analytics show common wrong answers), and to personalize support (e.g. flag learners at risk if they miss modules or fail attempts, then intervene). **Governance matters:** maintain version control on materials, schedule periodic reviews (at least annually) with SMEs to ensure content is up-to-date and relevant. By combining solid science, careful planning, engaging delivery, rigorous assessment, and continuous improvement, you will build an eLearning program that is scalable, impactful, and **“rock-solid.”**

## B. Program/Course Blueprint (Template)

*How to use this:* Use the blueprint table to align **each learning outcome** with appropriate activities, assessments, and completion criteria. This ensures nothing falls through the cracks – every outcome has evidence and tracking.

| Outcome (Learner will…) | Bloom Level | Evidence of Learning | Learning Activity | Assessment Method | Completion Rule | Competency | Analytics Signal |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **1\. Explain the core principles of effective team communication.** (e.g. list and describe key concepts) | Understand (B2)[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials) | Correct explanations of principles (e.g. quiz answers) | Interactive mini-lecture with examples; reading | 5-question MCQ quiz on principles | **Pass ≥80%** on quiz (max 2 attempts)[\[15\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=knowledge,is%20used%20to%20check%20for); otherwise, review material and retry | Communication Theory (Knowledge) | *Quiz score, attempts.* Flag if \<60% on 1st attempt (need support) |
| **2\. Demonstrate active listening skills in a simulated client conversation.** | Apply (B3) | Video/audio of learner performing listening techniques; behavior checklist | Role-play scenario simulation (with virtual client) | Performance assessment with **rubric** (coach or AI evaluates checklist) | **Complete simulation \+ score ≥3/4** on all rubric criteria (may repeat practice until successful) | Client Communication (Skill) | *Simulation logs:* completion timestamp; rubric scores per criterion. (Track improvement across attempts) |
| **3\. Analyze a workplace conflict case to identify root causes and propose a resolution.** | Analyze/Evaluate (B4/5) | Written analysis with correct identification of causes and resolution strategy | Case study problem (branching scenario or written case analysis) | Short-answer response or branched scenario choices; graded via rubric for logic and completeness | **Submit analysis** (mandatory) \+ rubric score meets minimum (e.g. ≥70%). If not, **remediation:** see model answer and submit revision. | Conflict Resolution (Competency) | *Assessment data:* rubric category scores (to see which aspect was weak); *Response path:* in branching scenario which choices made (for insight into misconceptions) |
| **4\. Create a personal communication improvement plan for the next 3 months.** (Capstone) | Create (B6) | Final written plan with goals and strategies | Capstone project (self-reflection and planning task) | Project deliverable (document or presentation) peer-reviewed or instructor-reviewed with rubric | **Submit plan** (completion required, qualitative feedback only) | Professional Development (Metacognitive) | *Completion status:* submitted/not. *Peer feedback* qualitatively analyzed (optional sentiment or keyword analysis) |

**Blueprint Tips:** Each outcome is specific and measurable (includes action verb, context, and criterion). The **Bloom** level guides the activity and assessment type (e.g. an “Analyze” outcome isn’t assessed with simple recall questions). Note how **activities and assessments align** to the outcome: e.g., Outcome 2 (skill) uses a realistic simulation and a rubric measuring that skill, while Outcome 1 (knowledge) uses an objective quiz[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials). **Completion rules** define what it means to master or complete that item (pass threshold or submission). These rules feed into course completion criteria (e.g. must achieve all outcomes). Each outcome ties to a broader **competency** (if using competency-based learning, link to competency frameworks). **Analytics signals** list what data to track for that outcome – use these in dashboards to identify if learners are meeting the outcome. For example, if many learners fail Outcome 3’s analysis on first try, that signals a gap in either the content or the learners’ prior knowledge; you might add an extra practice case or coaching.

*Common failure mode:* **Outcomes not aligned with assessment.** Prevent this by using the blueprint: for each outcome, explicitly decide how you’ll know it’s achieved (e.g. if outcome says “demonstrate X skill” but assessment is a quiz, that’s a mismatch – better to include a performance demo). Another pitfall is writing outcomes that are too vague or unmeasurable (e.g. “understand ethics”); instead, use concrete verbs and criteria (e.g. “Given a scenario, **identify** at least 3 ethical issues and **recommend** an action for each”). Ensure every outcome in the program appears somewhere in content and is tested – the blueprint table should have no orphan outcomes or orphan assessments.

## C. Assessment Blueprint (Plan)

*How to use this:* Plan each **assessment component** with this table to ensure balanced, fair testing aligned to your outcomes. Specify item types, difficulty mix, scoring, and policies for retries to uphold both rigor and learner support.

| Outcome Assessed | Item Types | \# of Items | Difficulty Mix | Scoring Method | Retake / Remediation Rules |
| :---- | :---- | ----: | :---- | :---- | :---- |
| **Outcome 1:** Basic concepts quiz *(e.g. end of Module 1\)* | 5 MCQs (single answer) | 5 | 2 easy (recall definitions), 2 moderate (understand scenario application), 1 challenging (edge case) | Auto-scored (each Q 1 point). 100% total \= 5 pts. | **2 attempts allowed.** If \<80% on 1st attempt, LMS shows hints on review; must retry after reviewing lesson. Record highest score. |
| **Outcome 2:** Simulation checkpoint *(during Module 2 practice)* | 1 branched scenario with 3 decision points (multi-step) \+ 2 reflection short-answer questions | 5 interactions total | Scenario path decisions: moderate difficulty (realistic client responses). Reflection Qs: higher-order (explain reasoning). | Auto-path in scenario (must reach successful outcome); reflections reviewed qualitatively by coach (for feedback, not points). | **Unlimited practice attempts** in simulation until “optimal path” is found. Reflection questions: *required* but ungraded; coach provides narrative feedback. |
| **Outcome 3:** Case analysis & quiz *(Module 3\)* | 1 open-ended analysis (essay) \+ 4 item quiz (mix: 2 MCQ, 1 multi-select, 1 matching) | 5 (1 item is essay, 4 objective) | Objective items: 1 easy, 2 moderate, 1 hard (cover common pitfalls in conflict ID). Essay: hard (requires synthesis of case factors). | Objective: auto-scored (each 1 point, 4 pts total). Essay: scored via rubric (each of 4 criteria rated 1-4; total scaled to 10 pts). Combined score out of 14\. | **One retake** allowed for objective quiz part (different Qs drawn from bank). Essay: **one revision** allowed after feedback if initial score \<70%. Final grade averages attempts. |
| **Outcome 4:** Capstone project *(end of course)* | Project deliverable \+ peer-review rubric (analytic rubric, 4 criteria) | 1 project | Task itself is complex (requires creative application – high difficulty). Peer reviews: peers use rubric guiding from exemplary to poor. | Scoring: For learning purposes, project is graded Complete/Incomplete by instructor (completion \= submitted \+ peer reviews done). Peer rubric feedback given for improvement (qualitative). | **No retake** (one submission). However, learners get two peer reviews and can optionally revise their plan after the course for their own benefit. |

**Assessment Design Notes:** We mix question types to cover learning outcomes appropriately: \- **Objective items (MCQ, multiple-select, matching, numeric)** are great for facts and basic concepts (Outcome 1). We include a range of difficulty: easy items to build confidence and ensure coverage of basics, and a couple of hard items to stretch top learners and discriminate understanding[\[16\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20the%20example%20above%2C%20where,Even%20more%20dramatically%2C%20after%20one). A typical target is \~20-30% easy, \~50% medium, \~20-30% hard questions, calibrated via pilot results (item analysis). \- **Higher-order outcomes** (like analysis in Outcome 3\) use open-ended items and scenarios. Here we combined an essay with a few targeted objective questions addressing common mistakes. The essay is scored with a rubric to maintain consistency and fairness. \- **Authentic assessment:** Outcome 2 and 4 focus on performance. The simulation (Outcome 2\) provides a safe environment to apply skills, with immediate feedback branching. The capstone (Outcome 4\) is a real-world task (creating a plan) evaluated more for completion and insight than a numeric score – suitable for a summative, integrative outcome.

**Retake rules** are tailored to the stakes and learning goals: \- For low-stakes knowledge checks, allow 2 attempts and give hints after the first attempt – this promotes retrieval practice while guiding learning (failure is treated as feedback, not the end). \- For high-stakes exams, one retake with different questions can be allowed if mastery is critical, but typically with an remediation step (e.g. require reviewing specific content first). \- For performance tasks, often no formal “retries” in the same run, but you might allow revisions or a follow-up assignment if not satisfactory. In Outcome 3’s essay, we allow one revision after feedback, since writing is a skill improved by iteration and feedback (a form of **feedforward**). \- **Mastery logic:** If a learner still fails after allowed retries, have a remediation plan (additional coaching, or mark as incomplete requiring instructor intervention).

*Common failure mode:* **Poor item quality undermines assessment.** To avoid this, follow item-writing guidelines (see Checklist below) – e.g. ensure each MCQ has one unambiguously correct answer, avoid trick questions or “All of the above” options that let test-wise learners game the test[\[17\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,solving%20and%20creativity)[\[18\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=%E2%80%A2%20Students%20merely%20need%20to,students%20know%20the%20correct%20answer). Another pitfall is misaligned difficulty – e.g. all easy questions for a complex skill will inflate scores without true mastery. Use an **assessment blueprint** (like above) to intentionally design difficulty distribution and item types that match the outcome’s cognitive level. After piloting, examine item statistics: if an item is too easy (almost everyone gets it right) or too hard (very few get it right), or if it doesn’t correlate with overall test score, consider revising or replacing it. Regularly review that **assessments map to outcomes** (alignment): each question/task should trace to a specific learning objective – if it doesn’t, question why it’s there.

## D. Item-Writing Checklist \+ Example Items

*How to use this:* Before finalizing quizzes/exams, run through this checklist to catch common item-writing issues. The example items illustrate various question formats (multiple-choice, matching, etc.) for both compliance and skills contexts, serving as models for your own item creation.

### Item-Writing Quality Checklist (for objective items)

* **✅ One clear problem:** Each question should assess **one** focused concept or skill. Learners should understand the problem from the stem alone (avoid requiring them to read all options to get context)[\[19\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=11,%E2%80%9CNone%20of%20the%20Above%E2%80%9D%20Option).

* **✅ Plausible distractors:** For MCQs, all incorrect options (distractors) must be credible. Use common misconceptions or errors. Avoid absurd or joke options that give away the answer.

* **✅ Mutual exclusivity:** Options should not overlap in meaning. Ensure only one **best answer** (unless it’s a “Select all that apply” type, which should be clearly indicated).

* **✅ Clear wording:** Use simple language. Avoid negatives (“Which of these is *not*…”) as they can be misread. If you must use a negative, **bold** or capitalize it (“NOT”) for emphasis.

* **✅ No unintended clues:** Keep option lengths and formats uniform (one option shouldn’t stand out as longer or more specific). Avoid grammatical giveaways (e.g. “an **a**nswer” revealing the vowel start of correct option). Shuffle the position of correct answers randomly.

* **✅ “All/None of the above” caution:** Generally **avoid** these options[\[17\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,solving%20and%20creativity). “All of the above” can be guessed by partial knowledge[\[20\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,%E2%80%9CNone%20of%20the%20Above%E2%80%9D%20Option), and “None of the above” doesn’t test positive knowledge (a learner could choose it without knowing the correct answer)[\[21\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=answer%20correct%2013,Item%20Types%20Are%20More%20Appropriate).

* **✅ Appropriate difficulty:** Match the question to the learning level. Use scenarios for application/analysis level, direct questions for recall. Calibrate difficulty by pilot testing if possible (look at percentage correct).

* **✅ One correct answer per question (for single-response):** Sounds obvious, but double-check that none of the distractors could be arguably correct. Conversely, for a multi-select, ensure you specify “Select all that apply” and have an acceptable answer key for all correct combos.

* **✅ Consistent style:** If using a certain format (like all MCQs have 4 options labeled A–D), stick to it for a professional look. Randomize option order when possible to avoid bias (LMS can do this automatically).

* **✅ Technical accuracy:** For numeric or fill-in answers, anticipate format variations (e.g. accept “5” and “5.0” if appropriate). In matching, ensure one-to-one matches and equal list lengths (or clearly state if options can be reused).

* **✅ Bias and sensitivity:** Review items to ensure content is culturally appropriate and free of bias or stereotypes. Avoid idioms or context that could disadvantage learners from different backgrounds.

### Example Assessment Items (with Correct Answers)

1. **Multiple-Choice (Compliance topic):** *Which of the following* *password practices* *best aligns with company cybersecurity policy?*  
   A. Using the same password for all accounts (easy to remember)  
   B. **Using a unique passphrase with at least 12 characters for each account** *(Correct)*  
   C. Writing passwords on a sticky note near your workstation  
   D. Sharing your password only with your manager  
   *Why?* – Tests understanding of policy; B is correct as it reflects strong unique passwords.

2. **Multiple-Select (Compliance topic):** *ABC Corp’s Code of Conduct outlines several* *conflict of interest* *scenarios. Which TWO of the following would* *likely be considered* *a conflict of interest? (Select 2\)*

3. \[ \] An employee starts a side business that competes with ABC Corp. **(✓)**

4. \[ \] An employee’s family member works for a competitor. **(✓)**

5. \[ \] An employee attends a professional conference during work hours with manager approval.

6. \[ \] An employee receives an award from a industry association.  
   *Correct:* First two are conflicts. *Why?* – Multi-select requires recognizing both correct situations from plausible options. (The last two are not conflicts given the context.)

7. **Matching (Technical training):** *Match each* *HTTP status code* *with its meaning:*

8. 200 – A. **OK – request succeeded**

9. 404 – B. **Not Found – resource doesn’t exist**

10. 500 – C. **Internal Server Error – server encountered an error**

11. 301 – D. **Moved Permanently – resource URI changed**  
    *Answer:* 200-A, 404-B, 500-C, 301-D.  
    *Why?* – Tests recall of technical codes. Matching format reduces language load and is efficient for related pairs.

12. **Fill-in-the-Blank (Skills training):** *In the context of project management, the acronym RAID stands for managing four types of items: Risks, Actions, Issues, and \_\_\_\_. (One word)*  
    **Decisions** *(Correct)*  
    *Why?* – Direct recall of a key concept. The one-word answer (“Decisions”) is clearly correct or incorrect, making it auto-gradabe.

13. **Numeric Calculation (Finance training):** *If an investment of $1,000 grows by 5% annually, what will its value be at the end of 2 years (rounded to the nearest dollar)?*  
    **$1,102** *(Correct calculation: 1000 \* 1.05^2 \= 1102.50 ≈ $1,102)*  
    *Why?* – Checks ability to apply a formula. Numeric entry is appropriate; we specify rounding instructions to avoid ambiguity.

14. **Short Answer (Product knowledge):** *“List three key features of the new XYZ software that would* *benefit a sales team.”*  
    **Correct example answer:** 1\) Real-time lead notifications, 2\) Mobile CRM access, 3\) Analytics dashboard for tracking targets. *(Answers may vary; grader looks for any three features related to sales enablement as per training materials.)*  
    *Why?* – Short answer lets learners recall and articulate key points in their own words, showing depth of understanding beyond recognition.

15. **Essay with Rubric (Leadership training):** *“Describe a difficult team situation you have faced* *and how you applied at least two leadership principles from this course to resolve it. What was the outcome, and what did you learn?”*  
    *Rubric stub:* This essay would be graded on **Criteria:** a) Identification of relevant leadership principles (0-3 pts), b) Application of principles to the situation (0-3 pts), c) Reflection on outcome/lesson (0-3 pts), d) Clarity and coherence (0-1 pt). *(Total 10 pts)*  
    *Why?* – An essay prompts real-world application and reflection, crucial for leadership skills. The rubric ensures consistent grading and gives learners structured feedback on different aspects (knowledge, application, insight, communication).[\[22\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8DBest%20Practice%3A%20Levels)

16. **Scenario-Based Single-Choice (Compliance training):** *Scenario:* You observe a colleague frequently leaving confidential client files unlocked on their desk. According to policy, what *should you do first*?  
    A. Ignore it; it’s not your responsibility.  
    B. **Remind the colleague of the policy and encourage securing the files.** *(Correct)*  
    C. Immediately report the colleague to HR.  
    D. Secure the files yourself quietly every time.  
    *Why?* – A realistic scenario with a nuanced correct action. Tests judgment in applying policy. Only one best answer (B) aligns with company guidelines (educate first, escalate if not resolved). Options are plausible to reflect common reactions.

17. **Branching Scenario (Sales skills):** *You are meeting a potential client who is hesitant about our product’s price. In the first meeting, they say, “It’s too expensive.”* **What do you do?**

18. **Choice 1:** Immediately offer a 20% discount. *(Leads to branch: client agrees to trial but you’ve set a low price expectation – suboptimal outcome.)*

19. **Choice 2 (Correct):** Ask open-ended questions to understand their concerns (“Can you share what aspects seem not worth the cost?”). *(Leads to branch: you uncover value gaps and can address them – optimal outcome.)*

20. **Choice 3:** End the meeting and follow up later via email. *(Leads to branch: client loses interest – failure outcome.)*  
    *Why?* – Branching scenarios test decision-making. The “correct” path is asking questions to handle the objection, demonstrating a taught consultative selling skill. Other branches provide learning moments and feedback (this would be part of the simulation with immediate consequences shown).

21. **Ranking/Ordering (Process training):** *Put the following steps of the incident response process in the correct order (1=first, 4=last):*

    * **Notify the incident response team**

    * **Contain the incident (e.g., isolate affected systems)**

    * **Eradicate the threat (remove malware, etc.)**

    * **Conduct a lessons-learned review**  
      **Correct Order:** 1\) Notify team, 2\) Contain, 3\) Eradicate, 4\) Lessons learned.  
      *Why?* – Sequence matters in processes. Ranking question type checks if learners know the proper order of operations. It’s automatically gradable if the LMS supports ordered list questions.

*Item Writing Tips in Action:* In the examples above, notice no question was a “trick question.” Wording is direct and positive. The multiple-choice items avoid “All of the above” traps and each distractor in Q1 and Q2 is reasonable (no obviously goofy throw-outs). For multi-select, the question clearly states how many to select. The matching question pairs are homogeneous (all are status codes, so it’s a fair matching set). We provided context where needed but kept it concise – e.g. the scenario question gives just enough detail to set up the decision. All items align with what was taught: e.g., the leadership essay explicitly asks for principles “from this course.” This alignment builds validity – we’re testing what we taught. Finally, each item was reviewed for accessibility: e.g., in the scenario, key info is in text (not image-dependent), and for any question referencing images or charts (none here, but if), ensure you provide alt text or descriptions.

## E. Rubric Template (Analytic, 4 Levels) \+ Example

*How to use this:* Adapt the rubric template for any project or performance assessment. **Analytic rubrics** mean each criterion is scored separately. We provide a blank template and then an example filled-in for a specific task. Copy-paste and modify criteria/levels to suit your needs. Use 4 performance levels (e.g. Exemplary, Proficient, Developing, Beginning) – research suggests 3-5 levels is ideal for clarity[\[22\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8DBest%20Practice%3A%20Levels).

**Blank Rubric Template (4-Level Analytic Rubric):**

| Criteria | Level 4 (Exemplary) | Level 3 (Proficient) | Level 2 (Developing) | Level 1 (Beginning) |
| :---- | :---- | :---- | :---- | :---- |
| *Criterion A* (e.g. Content Accuracy) | Completely accurate and comprehensive; no errors. | Mostly accurate with minor gaps or errors. | Some inaccuracies or omissions; partial understanding. | Major inaccuracies or misunderstandings. |
| *Criterion B* (e.g. Application of Skills) | Skill applied expertly in a highly effective way. | Skill applied appropriately with good effectiveness. | Skill application attempted but somewhat ineffective or incomplete. | Little to no application of the skill demonstrated. |
| *Criterion C* (e.g. Organization/Structure) | Exceptionally clear, logical, and well-structured. | Generally clear and logical; minor issues in flow. | Some structure evident, but lapses in clarity or logic. | Disorganized or unclear; difficult to follow. |
| *Criterion D* (e.g. Language/Presentation) | Consistently professional and engaging; no grammar or formatting errors; adheres to all guidelines. | Generally clear; few errors; follows guidelines with minor lapses. | Noticeable errors or lapses in professionalism; some guidelines not followed. | Unclear or inappropriate communication; many errors; ignores guidelines. |

*Using the template:* Replace placeholder criteria (A, B, C, D) with those relevant to the assignment. For each criterion, the descriptions under each level should describe what performance looks like. Make descriptors **observable and specific** – avoid just words like “Good” or “Poor” which are subjective[\[23\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8D)[\[24\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=Typically%2C%20students%20find%20most%20rubrics,what%20is%20expected%20of%20them). Instead, describe qualities (e.g., “includes 3+ supporting examples” vs. “lacks examples”). Keep criteria focused on the most important aspects of the work (too many criteria can overwhelm; 3-5 key criteria often suffice[\[25\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=Best%20Practice%3A%20Criteria)).

**Filled-In Rubric Example:** *For Outcome 2 “Active Listening Simulation” – an observer/coach rates the learner’s performance in a role-play of active listening.*

| Criteria | Level 4 – Expert Listener | Level 3 – Competent Listener | Level 2 – Basic Listener | Level 1 – Needs Improvement |
| :---- | :---- | :---- | :---- | :---- |
| **Body Language & Focus** (Maintains eye contact, no distractions) | Maintains eye contact, nods, and shows full engagement at all times. No distracting behaviors. | Generally attentive with rare minor distractions (e.g. occasional glance away). | Some signs of distraction (frequent looking at phone or around), inconsistent eye contact. | Often unfocused or distracted; poor eye contact; listener appears disengaged. |
| **Reflective Responses** (Paraphrasing and clarifying speaker’s points) | Consistently paraphrases speaker’s ideas to confirm understanding; asks insightful clarifying questions. | Paraphrases most key points accurately; asks clarifying questions when needed. | Infrequently paraphrases or misses some key points; questions may not clarify core issues. | Does not paraphrase or clarify; responses indicate misunderstandings or lack of active processing. |
| **Empathy & Tone** (Conveys understanding and respect) | Tone is fully respectful, empathetic, and patient. Listener validates speaker’s feelings explicitly. | Tone is respectful and understanding; listener occasionally validates speaker’s emotions. | Tone is generally polite but may lack warmth or occasional awkward responses; little emotional acknowledgment. | Tone can be dismissive or curt; little to no acknowledgment of speaker’s feelings. |
| **Following Up** (Uses appropriate follow-up questions or summaries) | Seamlessly builds on speaker’s points with relevant follow-up questions; summarizes conversation effectively at end. | Asks some relevant follow-up questions; provides a satisfactory summary or next steps. | Few follow-up questions, or some are off-topic; summary/closure is incomplete. | Does not follow up appropriately; conversation ends without summary or clear next steps. |

*Rubric usage:* In this example, each criterion targets a different aspect of the listening skill. The levels provide a clear continuum from excellent to poor performance. During evaluation, the coach would mark the level achieved for each criterion. An **analytic rubric** like this allows pinpointing strengths and weaknesses (e.g. a learner might be Level 3 on Body Language but Level 1 on Reflective Responses – indicating a specific gap to work on). Scores can be assigned (e.g. Level 4 \= 4 points, Level 3 \= 3, etc.) or you can use the rubric qualitatively without numeric grades, depending on context.

**Tips for rubrics:** Share the rubric with learners **before** they start the task so they know what is expected. This transparency guides their efforts (it “manages student expectations” and reduces ambiguity[\[26\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8D)). Ensure rubric criteria tie back to the learning outcomes (if a criterion doesn’t relate to an outcome, consider dropping it). Watch out for **common pitfalls:** too many criteria (dilutes focus), unclear level descriptors (if learners can’t distinguish what makes a Level 4 vs Level 3, the rubric won’t be helpful), or overly task-specific criteria (focus on learning outcomes quality, not trivial details like “Used 12 pt font” unless it’s relevant to the competency). Revise rubrics based on grader experience – if everyone is scoring high on a criterion because it was too easy, or if a criterion never gets a Level 4, adjust descriptors or expectations. A good rubric is a living document.

## F. Lesson Pattern Templates (Micro & Deep Sessions)

*How to use this:* These are **copy-pasteable lesson blueprints**. The first is for a quick-hit microlearning (15–25 min) session, the second for a deeper 45–90 min lesson or workshop. They ensure you include all key elements (from hook to wrap-up). Customize the timing and activities as needed, but maintain the flow to maximize engagement and retention.

### Template: 15–25 Minute Microlearning Lesson

* **Title & Hook (1–2 min):** *Introduce the lesson with a bang.* Grab attention using a provocative question, surprising fact, or a relevant scenario. (Example: **“What would you do if a client says ‘your price is too high’?”** – shows a real challenge to solve). This primes curiosity.

* **Learning Objectives (1 min):** *State what learners will achieve.* Keep it brief: display 1–3 bullet-point outcomes in learner-friendly terms (e.g. “You will be able to **handle a pricing objection** confidently”). Ensure objectives are specific and action-oriented.

* **Activate Prior Knowledge (2 min):** *Connect to what they know.* Use a quick poll, Q\&A, or mini-quiz to recall relevant info. (Example: a poll “Have you ever negotiated a price at work? (Yes/No)” or a question “Name one tactic you’ve used?”). Acknowledge answers to validate experiences and create relevance.

* **Mini-Exposition (5–7 min):** *Teach the new concept concisely.* Present content in a focused chunk. This could be a short video, a few slides with voice-over, or an infographic. Apply **multimedia principles**: e.g. simple visuals \+ narration, avoid text-dense slides[\[9\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=1). Keep the content tightly aligned to the objective (no tangents – remember cognitive load). If complex, break into 2 smaller chunks with an interaction in between.

* **Guided Practice (3–5 min):** *Learners try it with support.* Immediately after the content, pose a practice question or scenario. Walk through the solution or thought process. This could be a single **worked example** or a demo: e.g. show how to apply a formula, or demonstrate a skill. Encourage learners to attempt a step (perhaps ask them to jot an answer) before revealing the solution. Provide explanation on why the solution works (to build schema, reducing future extraneous load[\[27\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=,schema%2C%20particularly%20with%20novice%20leaners)).

* **Independent Practice (3–5 min):** *Now their turn, unassisted.* Give a question, problem, or interactive exercise covering the same skill. In a micro-lesson, this might be a **brief quiz (2–3 questions)** or an interactive scenario. For example, a one-question scenario: “Client says price too high – choose your reply:” with immediate feedback. Ensure **feedback** is given: if correct, reinforce (“Correct, because...”), if wrong, explain why and the correct answer (this is formative – *learning* more than evaluation).

* **Reflection (2 min):** *Prompt thinking about learning.* Ask learners to reflect or write a one-sentence takeaway: “What’s one thing you’ll do differently when facing a price objection?” This can be a rhetorical question or facilitated via a discussion board or chat (even in a self-paced module, you can prompt journaling or provide an example reflection).

* **Closing & Next Steps (1 min):** *Wrap up clearly.* Summarize the key point or model (e.g. “In summary, when faced with a pricing objection: **don’t discount immediately – ask questions to understand concerns**.”). Link to next steps or resources: e.g. “Next lesson we’ll cover negotiating long-term contracts” or “See reference guide for a checklist of objection-handling phrases.” End on an encouraging note (“You’ve practiced a crucial skill in under 20 minutes – great job applying the pricing strategy\!”).

*Notes:* A microlearning lesson is **tight**. Total time \~20 minutes. It focuses on one bite-sized topic. The emphasis is on a single learning outcome. Interaction is critical to keep attention – notice we included at least 2 interaction points (a guided example and a quiz question). This template is well-suited for mobile delivery: short video or text, a quick question, etc. Ensure each segment is as lean as possible (if something isn’t directly serving the objective, cut it or save for a longer session).

**Common pitfall:** Trying to stuff too much content into a micro-lesson. Avoid this – if you have more than one or two objectives, split into separate micro units. Also, watch out for cognitive overload: in a short lesson, stick to the essentials and use simple visuals to support the text/audio (leveraging dual channels but not overcrowding either[\[9\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=1)[\[28\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=4)). For engagement, don’t skip the hook – even a 15-min module benefits from motivation upfront (it signals why they should pay attention, tapping into adult learners’ goal orientation).

### Template: 45–90 Minute Deep-Dive Lesson

*(Use for extended eLearning modules or live-online classes/workshops – can be done synchronously or as a longer asynchronous module broken into parts.)*

* **Warm-Up & Hook (5 min):** *Engage and set the stage.* Start with an activity that grabs attention and activates existing knowledge. For instance, a case study teaser: present a short story or dilemma relevant to the topic and ask, “What would you do?” Alternatively, show a striking image or data point and discuss. If live, could be a breakout discussion or a poll to surface opinions. Aim to create a need-to-know feeling.

* **Objectives & Agenda (2 min):** Clearly outline what will be learned and the flow of the session. For a long session, it helps to map the journey (“First we’ll X, then Y, finally Z”). Ensure objectives are measurable. Example: “By the end of today, you will **design a basic marketing plan** and **critique an example plan** using our 5-point checklist.”

* **Recall Prerequisites (5 min):** *Check foundational knowledge.* Use a quiz or group brainstorm to revisit last lesson’s key points or any assumed knowledge. If gaps appear, address them briefly or have a quick recap slide. This ensures everyone is ready for new material (and prevents the “house of cards” if prior knowledge is shaky).

* **Content Blocks & Activities (3–4 x 10 min blocks \= \~40 min):** *Teach in chunks*, interspersing interactive activities:

* **Content Block 1 (8-10 min):** Present first sub-topic. Use rich media (slides, demonstration, or a guided tour of a process). Apply storytelling or examples to illustrate abstract concepts (this aids elaboration, attaching meaning). Keep a conversational tone to maintain interest.

* **Quick Activity 1 (5 min):** Right after block 1, do an activity: a poll, a quick case analysis, a “think-pair-share” if live (or an online forum question if async). This reinforces what was just covered. For example, after teaching a formula, have learners attempt a simple calculation.

* **Content Block 2 (8-10 min):** Next segment of content. Possibly build complexity. Keep using visuals appropriately (e.g. Gagné’s principle: incorporate **guidance** like highlighting key steps[\[29\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=What%20it%20means%3A%20Learning%20is,out%20the%20most%20critical%20elements)).

* **Practice Activity 2 (5-10 min):** A more involved practice. Maybe a group exercise or a simulation snippet. For instance, learners analyze a provided scenario in small groups and post their solution approach. If asynchronous, this could be a multi-step scenario in the LMS with feedback at each step.

* **Content Block 3 (8-10 min):** Present remaining content or advanced examples. At this point, cognitive load might be high, so try **dual coding** – e.g. show a process flow diagram while explaining it, to engage visual and verbal channels[\[9\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=1). Emphasize application and connect to prior blocks (“Remember the case we discussed; now see how this theory explains it...”).

* **Practice Activity 3 (5-10 min):** Another exercise for the new content. Could be an individual task like a quick quiz, or a creative activity (e.g. “draft a one-paragraph plan based on what you learned, then compare with a model answer”). Encourage sharing answers if live (promotes relatedness and peer learning).

* *(Optional)* **Content Block 4 \+ Activity:** If 90 minutes, you have room for one more cycle or a guest demonstration, etc. Always follow the pattern: content then application. For lengthy sessions, consider a **5-min break** for recharging attention around the halfway mark.

* **Integration Exercise (10–15 min):** *Synthesize and apply all pieces.* This could be a comprehensive case study or problem that ties together the lesson’s topics. For example, learners work through a complex scenario using all the techniques learned, either individually or in teams. If live online, use breakout rooms or collaborative docs; if async, present a case with multiple questions covering each objective.

* **Debrief & Discussion (5–10 min):** *Review the exercise and key learnings.* Have groups share their solutions or reasoning. Discuss any differences, corrections, or exemplary approaches. This is a chance to address misunderstandings. Also invite questions – ensure there’s time for learners to clarify doubts (they might have questions after applying concepts).

* **Knowledge Check (5 min):** *Formal formative assessment.* End with a short quiz or poll to gauge understanding of each objective. Could be 3-5 MCQs or a Kahoot\! if live (fun way to conclude). Immediate feedback on these questions helps learners leave with correct understandings. If many miss a question, you can clarify right then.

* **Reflection & Closing (5 min):** *Encourage reflection and connect to next.* Ask: “What is one insight you’re taking away?” – learners can write in chat or personal journal. Provide the **summary** of key points (perhaps a downloadable one-pager or mind map). Explain how this lesson fits into the bigger program (motivate them for future content: “Next, we build on this by tackling...”). End positively: acknowledge effort (“Today was intense and you all analyzed a tough case – well done\!”) and encourage applying the learning on the job (“This week, try out these techniques in your team meeting and see the difference.”).

*Notes:* The deep-dive pattern uses **several content-practice cycles**. This aligns with adult learning preference for **application** and not just lecture. By spacing interactions every \~10 minutes, we respect attention span limits and use **spaced repetition** within the session (coming back to concepts multiple times in different ways). The structure also supports **universal design**: various activities (visual, auditory, discussion, hands-on) address different learning preferences and needs – offering **multiple means of engagement and expression** for learners[\[30\]](https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning#:~:text=UDL%20calls%20for%20multiple%20means,on%20these%20categories%20see%C2%A0CAST%20website).

**Pitfall watch:** In long sessions, a common failure is drifting into a long lecture without interaction – combat this by planning the agenda with activities as above. Also monitor pacing: it’s easy to spend too long on one segment and rush later parts. Time-box your agenda and have less-critical content as “nice-to-have” that can be dropped or assigned as self-study if you run short on time. Always include the integration exercise and debrief, even if it means cutting a minor topic – these help consolidate learning (without them, learners leave with disjointed pieces). Lastly, ensure accessibility: for all materials (slides, documents, videos) used in the lesson, follow **WCAG** guidelines (e.g. readable fonts, high contrast, provide alt text, caption videos, and if live, describe any visual being discussed for anyone who might not see it). If group work is involved, plan how to include remote or disabled learners effectively (e.g. ensure the platform supports live captioning, offer text alternatives to spoken activities, etc.).

## G. Design & Build Runbook (Workflow)

*How to use this:* Follow this step-by-step **runbook** for developing an eLearning program from scratch. It’s platform-agnostic, focusing on tasks and decision points. For each phase, we note key actions, recommended platform settings (generically), and how to incorporate governance (reviews, version control). Use this as a project plan checklist to ensure a smooth course development lifecycle.

**1\. Needs Analysis & Kickoff**  
\- **Define Audience & Constraints:** Identify who the learners are (roles, prior knowledge, tech access) and any constraints (timeline, budget, compliance requirements). *Assumption example:* “Learners are sales reps, on the road – need mobile offline access.” Document these. \- **Define Business Goals:** Clarify why the training is needed in business terms (e.g. “reduce onboarding time by 30%” or “ensure 100% compliance with safety regs”). These will guide success metrics[\[13\]](https://xapi.com/kirkpatrick/#:~:text=4%20Results%20What%20was%20the,have%20ready%20access%20to%20it). Engage stakeholders to get alignment on goals. \- **Assemble Team & Roles:** Who is the **Subject Matter Expert (SME)** (content authority)? Who is the instructional designer, developer, project manager, etc.? Establish a communication channel and frequency (e.g. weekly check-ins). \- **Project Plan:** Create a high-level timeline (possibly refer to the 90-day plan in section I). Include key milestones (design sign-off, content complete, pilot test, etc.). Set up a **version control system** for assets (it can be as simple as a shared folder with versioned filenames or a tool like Git/LMS built-in versioning for course objects). Also plan where you will track tasks/bugs – maybe a Trello board or project management tool.

**2\. Design Phase (Outcome & Content Blueprinting)**  
\- **Define Learning Outcomes:** Write clear, measurable outcomes for the program and each course/module. Use Bloom’s revised taxonomy for appropriate verbs and levels. Each outcome should tie to the business goals (traceability). Ensure SME and stakeholders sign off on outcomes – this is critical to avoid scope creep later. \- **Design Assessments First:** For each outcome, decide how you’ll assess it (following the **Assessment Blueprint** in section C). Example: Outcome “Apply safety rules” → assessment: “Workplace safety simulation with scored decisions”. Determine the question types, approximate number of items, and any real-world performance tasks. This is the **Backward Design** principle[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials): assessments aligned to outcomes before content creation. \- **Design Learning Activities & Content:** Outline the content and activities needed to prepare learners for those assessments. Make a **course map** (perhaps in a spreadsheet or using the blueprint table from section B) mapping each outcome to modules, lessons, practice activities, and assessments. This is effectively your **curriculum outline** or storyboard at a high level. Example entry: “Module 2: Outcome \= XYZ skill; Content \= video \+ demo; Activity \= practice quiz; Assessment \= skill demo via upload.” \- **Storyboard Lessons:** For each lesson or module, create a storyboard or lesson plan. This can be a slide deck or document listing text, visuals, interactions, etc., in sequence (the Lesson Pattern templates in section F guide this). Involve SMEs here to input/content review – they should validate technical accuracy and provide real examples or case studies (SMEs are crucial for authentic scenarios). \- **Apply Learning Design Frameworks:** Check your design against best practices: \- **Chunking:** Is content broken into logical, small chunks? (A module shouldn’t attempt to teach 10 huge concepts at once). \- **Cognitive Load:** Are we minimizing extraneous load? e.g., not including irrelevant info or decorative media[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2); each graphic has a learning purpose. \- **UDL & Accessibility:** Are there alternatives for different learners? (e.g. transcripts for audio, visuals with descriptions)[\[31\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=,drop%20or%20color)[\[30\]](https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning#:~:text=UDL%20calls%20for%20multiple%20means,on%20these%20categories%20see%C2%A0CAST%20website). Plan for captioning videos, ensuring color choices are high-contrast, etc. \- **Engagement:** Each module should have frequent interactions (see lesson patterns). Use a variety (polls, drag-drop, simulations, discussions) to keep interest. \- **Feedback strategy:** Decide where you’ll give feedback and hints. E.g. immediate feedback in quizzes; model answers for open-ended practice; coaching feedback for assignments. \- **Desirable difficulties:** Incorporate strategies like spaced retrieval (maybe plan spaced quizzes or recap sections), interleaving (mix practice from prior topics in later modules), etc., as appropriate[\[5\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20a%20recent%20study%2C%20nearly,term%20mathematics%20learning)[\[6\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=Researchers%20refer%20to%20the%20benefits,and%20practice%20what%20they%20know). \- **Peer Review Design:** Conduct an internal review of the outline and storyboards. Invite another instructional designer or a knowledgeable colleague to critique: Are outcomes clear? Is content logically ordered? Does the design align with adult learning principles? Catch issues early (cheaper to fix on paper than in development).

**3\. Development Phase (Building the Content)**  
\- **Set Up LMS Structure:** In the LMS or authoring tool, create the course shell, modules/topics structure, and navigation. Set up a **consistent template** for lesson pages if possible (branding, header, fonts). Configure basic settings: \- **Enrollment settings:** e.g. self-enroll vs. auto-enroll learners, dependencies if any (prerequisites to unlock modules? mastery paths where failing triggers remedial content?). \- **Completion tracking:** enable completion tracking for activities (so you can mark modules complete when conditions met, like achieving quiz pass score). \- **Grading scheme:** set up grade categories (quizzes, assignments, etc.) as needed. If using mastery grading, configure pass/fail or competency scales. \- **Develop Content Assets:** Create videos, write text content, design slides, record audio. Follow best practices for each media: \- Videos: Keep them short (ideally \<6 minutes each for one idea). Include captions (auto-generated then edited for accuracy)[\[32\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=1,A). Ensure visuals in videos have narration describing them (for vision-impaired). \- Text: Write in plain language, concise paragraphs (use our 3-5 sentence guideline per paragraph). Use headings and bullet lists for readability (like this playbook does). \- Graphics: Make diagrams or illustrations to explain concepts (check for colorblind-friendly palettes and add alt text). Avoid irrelevant images that don’t support learning (coherence principle)[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2). \- Interactive elements: Develop any slide interactions, simulations, or branching scenarios using authoring tools (Articulate Storyline, Captivate, etc., or in-LMS tools). Test the decision logic thoroughly. \- **Build Assessments:** Using your assessment plan, create question banks in the LMS. For quizzes: \- Configure **question banks/pools** so you can randomize questions (e.g. draw 5 out of 10 questions for a quiz to minimize cheating/sharing answers). \- Set **randomize answer choices** on MCQs (and ensure “None of the above” if used is not always last due to randomization or else lock its position appropriately). \- Apply **point values** to each question as planned and set overall quiz passing score accordingly. \- Add **feedback for answers** where applicable (especially for formative quizzes: brief explanations for why an answer is correct or not). \- If the LMS supports it, set **attempt limits** and **scoring rules** (e.g., highest attempt or average) as per blueprint. For example, allow 2 attempts, record highest score. \- For open-ended assignments: set up the submission dropbox. Attach the rubric (if LMS has a rubric tool, input the rubric criteria and levels). Decide if peer review is used – configure peer review settings (anonymous or not, how many peers). \- For discussion-based assessments, set clear instructions and consider using grading rubrics for quality of posts if needed. \- **Quality Checks during build:** As content is assembled, do iterative reviews: \- *SME content review:* Ensure the SME reviews each lesson’s content for accuracy and completeness. Check that examples are valid and up-to-date. \- *Style and accessibility review:* Use tools like an LMS accessibility checker or browser plugins (WAVE) to scan pages. Check heading structure, alt text on images, link text (no “click here” – use descriptive links[\[33\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=2,A)). \- *Functional testing:* Preview each lesson and quiz as a learner. Verify that videos play, links work (set them to open in new tabs to avoid losing LMS page[\[34\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=,suggested%20videos%20when%20it%20ends)), and interactive elements function. Ensure navigation is intuitive (e.g., next buttons go to the right place; modules unlock as intended). \- *Performance optimization:* Especially if users have varying bandwidth, make sure media files are optimized (compressed video, not huge images). Consider offering a lower-bandwidth alternative (transcript or PDF of key content) for those on slow connections. \- **Version Control:** As you develop, keep track of versions of each asset. If multiple team members, consider using a naming convention (e.g. Draft\_v1, v2\_final) or a simple repository. Document major changes in a change log (this helps later if you need to audit what changed when, useful for compliance courses that require version history).

**4\. Testing & QA Phase** (overlaps with development – continuous testing, then a final QA pass):  
\- **Internal QA Testing:** Have a person not involved in development do a run-through of the entire course (or each module). Use the **QA Test Plan** (section H) as a guide: check every scenario like a learner who meets prerequisites, one who doesn’t, someone failing a quiz, etc. Verify completion rules trigger correctly (e.g., failing a quiz hides the next module until retry). Check analytics tracking if possible on a test account (does the LMS record scores, completion status? If using xAPI, are statements properly sent?). \- **Accessibility Testing:** Test with a screen reader (e.g. NVDA) for a few pages to ensure the reading order and alt text make sense. Try keyboard-only navigation: can you tab through interactive elements and activate them without a mouse? Ensure no critical activity is drag-and-drop without an alternative (WCAG says provide an alternative for drag-drop since not keyboard accessible by default)[\[35\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=,drop%20or%20color). \- **User Acceptance Testing (UAT):** If possible, run a small UAT with a handful of target learners or stakeholders. Have them take the training and complete a survey or interview after. They might catch usability issues or content that’s unclear to novices (SMEs sometimes miss that something isn’t common knowledge). Ensure we capture their feedback systematically (e.g., have a feedback form or the QA defect tracker for them to log issues). \- **Finalize Content:** Fix any issues found during QA/UAT. This may iterate a couple times for stubborn bugs or content tweaks. When all test cases pass and stakeholders sign off, you’re ready for deployment.

**5\. Deployment & Rollout**  
\- **Pilot Launch:** Start with a pilot group (if applicable – sometimes UAT and pilot are combined). For example, roll out to one department or a small percentage of the audience. Monitor closely: check LMS data to see if they are progressing as expected, solicit their feedback. This pilot acts as a last safety net to catch any issues in a real environment (e.g., firewall blocked content, or misinterpretation of an assessment question). \- **Analyze Pilot Data:** Look at quiz scores, time taken in modules, feedback comments. Are there any **red flags**? For instance, if 80% of pilot users failed a particular quiz question, that question or the content might need revision (maybe it was ambiguously worded or the content didn’t prepare them well). Address necessary changes quickly. \- **Scale Rollout:** Enroll all target learners. Typically, send a kickoff communication (email from leadership stressing importance, etc.). In the LMS, ensure notifications are set (a welcome message, due dates if any with reminder emails). \- **Support & Community:** Set up support channels: e.g., an FAQ page and an email/Slack for help. Possibly create a discussion forum in the course for learners to ask questions and share insights (moderated by an instructor or SME). This fosters a learning community and relatedness (motivational per SDT)[\[11\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=3,it%20motivates%20them%20to%20succeed). \- **Monitor & Engage:** Throughout the rollout, track progress analytics. Many LMSes have dashboards – see how many have started, who is falling behind. Send nudges as needed (automated or manual): e.g., “We noticed you haven’t logged in recently – need help?” for those inactive, or “Congrats on finishing Module 1\!” to those who did (positive reinforcement). Use the **Analytics signals** defined earlier: for example, if a learner fails two quizzes in a row, flag and offer a tutoring session. \- **Collect Feedback:** After learners complete the program, collect Level 1 (reaction) feedback via a survey. Also, if possible, do Level 3 checks a few months later (ask managers if behavior improved, etc.). Plan this in the rollout schedule. Use this data for continuous improvement.

**6\. Post-Launch Governance & Maintenance**  
\- **Evaluation against KPIs:** At the 30-60-90 day marks post-launch, evaluate success metrics (are we seeing the business goal improvement, e.g., a drop in incidents after a safety course?). For learning metrics, analyze the full data: average scores, completion rate, drop-off points. Compile a **report for stakeholders** demonstrating impact (and areas to tweak). \- **Content Maintenance Plan:** Establish how the course will be kept current. For example, schedule SME review every year or whenever a policy changes (whichever sooner). Keep a maintenance log of updates. Version the course in LMS if needed (e.g., “v2025” tag). \- **Continuous Improvement:** Feed the findings from analytics and feedback back into design modifications. If analytics show a particular module took everyone twice as long as estimated, investigate why (was it too dense? technical issues?). If feedback says “too theoretical,” consider adding more examples or practice. \- **Archival/Retirement Policy:** Decide how long this course will run before re-development or retirement (especially for technology subjects that evolve). When retiring or replacing with a new version, plan to archive the old one (for record-keeping, especially if needed for compliance audits) and communicate to users.

**Recommended Platform Features & Settings:** *(applicable across many LMS/LXP systems)*  
\- **For Content Delivery:** Use LMS modules or learning paths to structure the content flow. Enable **prerequisites** or **adaptive release** (e.g., Module 2 only opens when Module 1 quiz passed – supports mastery progression). \- **For Quizzes:** Use question banks and randomization as mentioned. Enable **timers** only if appropriate (e.g., maybe on final exam to simulate certification exams, but not on practice quizzes to reduce anxiety). Always ensure **feedback** options are on for formative quizzes. For summative, decide if you show answers after attempt or only after quiz closes to prevent sharing – many LMS allow controlling answer review. \- **For Discussions/Collaborative work:** If using forums, set them to “must post before seeing others’ posts” to encourage original contributions. Use groups if the cohort is large, so discussions are in smaller sets (e.g., 100 learners split into groups of 10 for manageability and more intimate discussion). \- **For Assignments:** Turn on plagiarism checking if it’s writing-heavy and academic integrity is a concern. Use the LMS **rubric attachment** so learners can view the rubric with the assignment description. If peer review, check the workflow (some systems automatically distribute submissions to peer reviewers). \- **Analytics Tools:** If available, enable **learning analytics dashboards** or xAPI data streams. For example, set up xAPI statements for critical actions (quiz passed, video watched fully, etc.) to feed into an LRS (Learning Record Store) for deeper analysis[\[36\]](https://xapi.com/kirkpatrick/#:~:text=xAPI%20makes%20it%20easier%20for,the%20learning%20experience%20and%20other). Use LMS reporting to schedule progress reports to managers if needed. \- **Gamification (optional):** If appropriate for your audience and LMS supports it, you can enable badges or points for completion, high scores, etc., to boost engagement. But use gamification thoughtfully – align badges with meaningful achievements (e.g., a badge for completing all modules with 100% scores could motivate competitive folks, but ensure it doesn’t demotivate those who only get 80%).

**Risk Register & Mitigations:** Keep a log of potential risks throughout design and deployment: \- *Risk:* **Content drift** (course content becomes outdated or deviates from intended outcomes over time). *Mitigation:* schedule regular SME reviews; tie content pieces to sources with dates so it’s easier to spot outdated info; maintain an “update needed” flag system if policy changes occur. \- *Risk:* **Outcome–Assessment misalignment.** *Mitigation:* perform alignment check at design phase (via blueprint) and again after pilot (use item analysis to see if perhaps an outcome wasn’t well assessed or taught). Adjust either content or assessment accordingly. \- *Risk:* **Low learner engagement/adoption.** *Mitigation:* involve learners early (user research in needs analysis), secure leadership support to emphasize importance, use interactive design. Monitor engagement in first weeks and send reminders or tweak communication if uptake is low. \- *Risk:* **Technical issues at launch** (e.g., videos not loading for some). *Mitigation:* beta test on multiple devices/browsers; host videos on a stable platform; have IT on standby during launch week; provide a troubleshooting FAQ to learners. \- *Risk:* **Analytics blind spots.** (Not collecting data you need, or misinterpreting it). *Mitigation:* define key metrics during design (as we did with analytics signals). Ensure LMS or LRS is configured to capture them (test data flow). If something is not captured (like how many attempts on a specific scenario), see if you can add an xAPI trigger or at least a manual log (even asking learners in feedback). When analyzing, be cautious: e.g., low quiz scores might mean poor content, not just poor learners – combine quantitative data with qualitative feedback to diagnose. \- *Risk:* **Vendor lock-in / portability.** If the course content is created in a proprietary authoring tool or LMS, it could be hard to move. *Mitigation:* publish content in standard formats (SCORM, xAPI) when possible; keep an offline copy of raw media and text. If switching platforms, pilot migrating a module early to gauge effort. \- *Risk:* **Scope creep.** Stakeholders may want to add content beyond original outcomes (common if SMEs get enthusiastic). *Mitigation:* use the signed-off outcomes document as a contract. If new needs emerge, do a change request process: assess impact on timeline and whether it aligns with goals. It’s okay to iterate, but guard against bloating the course with “nice to know” info that overwhelms learners.

In summary, this runbook emphasizes *planning, alignment, testing,* and *iterative improvement*. Follow it and you’ll navigate from idea to launch systematically, reducing fire-drills at launch. Always remember to keep the learner experience central (would *you* enjoy taking this course?) and the business goal in sight (will this course actually improve X metric?). If you do that, the eLearning program will deliver real value.

## H. QA & Pilot Test Plan

*How to use this:* Use this plan to systematically test your eLearning course **before full launch**. It covers test scenarios, expected results (acceptance criteria), and how to log issues. This ensures the course works as intended for all users (including those with accessibility needs) and meets the requirements. The plan is organized so you can tick off each test case or note defects in a tracker.

**Test Scenarios & Acceptance Criteria:**

1. **Content Load & Navigation:**

2. *Scenario:* Launch each module/lesson as a learner.

3. *Expected:* All text, images, and videos load quickly and correctly. No broken images or dead links. Navigation buttons (Next, Back, Menu) function and follow the intended sequence. The table of contents highlights current section. If a lesson has subsections or tabs, ensure each can be accessed.

4. *Acceptable if:* Every content page can be reached and displays as designed within 3 seconds on a standard connection; interactive elements (accordions, pop-ups) open and close properly.

5. **Multimedia & Interactivity Check:**

6. *Scenario:* Play all multimedia (videos, audio) and interact with activities (drag-and-drop, clickable animations).

7. *Expected:* Videos play with clear audio; controls (pause, seek) work. Captions can be toggled on and are accurate/synced. Audio clips play fully. Interactive widgets respond (e.g., drag-and-drop returns feedback or result).

8. *Acceptable if:* No media file fails to play or throws error. Captions and transcripts are provided for AV content[\[32\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=1,A). Interactive exercises yield correct feedback and do not freeze or behave erratically.

9. **Assessment Functionality (Quizzes/Assignments):**

10. *Scenario:* Attempt each quiz under different conditions: one scenario where answers are all correct, one with some wrong answers, one failing entirely. Submit assignment uploads, discussion posts if applicable.

11. *Expected:* Quizzes calculate scores correctly, display the correct feedback per answer as configured. If multiple attempts allowed, the LMS permits retake after the set conditions (e.g., only after passing a certain time or after reviewing feedback). Once passed, if course design says module unlocks, verify that next module indeed unlocks.

12. *Acceptable if:* Scoring algorithm matches design (e.g., if a question is 2 points, the total reflects it). Gradebook records each attempt as per policy (highest or average). For open-ended submissions, file upload works (upload a test file) and confirmation is shown. Discussion forums allow posting and replies properly.

13. **Adaptive/Conditional Logic:**

14. *Scenario:* If you have branching or conditional content (like pre-test that exempts some modules, or content that only appears based on role or choices), test each path. E.g., for a branching scenario, intentionally make different choices to see different slides. If there’s a pre-assessment that can skip a module, do one test passing it and one failing it.

15. *Expected:* The course adapts correctly: branches show the right content for each choice; skipped content is indeed bypassed when conditions met. No dead ends (where a user could get stuck with no path forward).

16. *Acceptable if:* All conditional rules execute as designed. E.g., a mastery path: failing Module 1 quiz triggers a remedial content module – verify that appears and that the user cannot proceed to Module 2 until they complete remedial. Conversely, a user who passes doesn’t see remedial and can continue. Branching scenarios eventually converge or end with feedback; no broken branch links.

17. **Accessibility & UDL Checks:**

18. *Scenario:* Test key pages with a **screen reader** (e.g., NVDA or VoiceOver). Navigate via keyboard only (Tab, Enter) through the course interface and content. Try resizing text or using browser’s zoom to 200%. If possible, test with high contrast mode or a color-blindness simulator.

19. *Expected:* Screen reader announces content in logical order (heading first, then body, etc.). All images have descriptive alt text or are marked decorative as appropriate[\[37\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=1.1.1%20Non). Form fields or interactive controls have labels (the screen reader should announce what a button or input does). Keyboard navigation can activate all buttons/links (e.g., hitting Enter on “Next” proceeds). No content is lost or overlaps at 200% zoom (responsive design holds). No color is solely relied on to convey meaning (for example, instructions don’t say “correct answers in green” without another indicator).

20. *Acceptable if:* The course meets WCAG 2.1/2.2 AA standards for core criteria. Captions/transcripts are present, focus order is intuitive, no keyboard traps (where one can’t escape an element via keyboard). If any minor issues (like a skip in reading order that doesn’t hinder understanding), log them for fix if time permits or note for future. Accessibility must be high priority for any blocking issues (e.g., a blind user cannot access a quiz due to untagged elements is *not* acceptable).[\[38\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=Step%201%3A%20Understand%20What%20WCAG,2%20Is%20All%20About)

21. **Multi-Device Testing:**

22. *Scenario:* Open the course on multiple browsers (Chrome, Firefox, Safari, Edge) and devices (desktop, tablet, smartphone). If mobile app or offline access is promised, test those too.

23. *Expected:* The course content adjusts layout for different screen sizes (responsive design), or if using a mobile app, content is still navigable. No critical feature is unusable on mobile (e.g., if drag-drop doesn’t work on mobile, ensure there’s an alternative like dropdowns).

24. *Acceptable if:* On a smartphone, text is readable without horizontal scrolling, interactions can be done (if not ideal, at least functional). No browser-specific errors (like a button works in Chrome but not in Safari).

25. **Edge Case Scenarios:**

26. *Scenario:* Test unusual but possible user behaviors. E.g., leave a quiz in progress by closing browser – does it resume properly or gracefully prompt login? Double-submit an assignment – does it overwrite or give error? Time-out: if session times out, is the user informed and able to continue? Also test with a learner at different access levels (if roles: student vs manager view).

27. *Expected:* The system handles interrupts or repeats without data loss or confusion. If a session times out, user can log back in and continue roughly where left. Duplicate submissions either overwrite or are prevented with a clear message. Role-based content (if any) shows correctly for the appropriate role.

28. *Acceptable if:* No data corruption or major confusion results from these actions. It’s okay if the course asks “Are you sure you want to exit quiz?” or if it restarts the quiz on re-login as long as it’s consistent with design.

**Defect Tracking:** For any issues found in scenarios above, log them in the defect tracker. Use a simple template like:

| ID | Issue Summary | Scenario (where it occurred) | Severity (High, Med, Low) | Status | Owner | Comments/Resolution |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| 1 | Image on Module 2 page 3 not loading (broken link) | Content Load – Module 2 Page 3 | Med | Open | Dev (Alice) | Possibly incorrect file path. Alice to fix link and re-upload image. |
| 2 | "Next" button unresponsive after Quiz 1 if user failed twice | Assessment – Quiz 1 failure branch | High | Open | Dev (LMS admin) | Likely conditional release logic issue. Investigating rules. |
| 3 | Screen reader reads table columns in wrong order | Accessibility – Module 3 table | Low | Open | ID (Bob) | Add ARIA labels or simplify table for screen reader. |

*Severity definitions:* High \= user cannot progress or major functionality break, needs fixing before launch. Medium \= issue causes inconvenience or confusion but there’s a workaround, fix if possible. Low \= minor issue (typo, slight layout glitch) that doesn’t impede learning, fix if time allows or post-launch.

During QA, each issue gets an ID, described, and assigned to someone to fix. Track the status (“Open”, “Fixed – needs retest”, “Closed”). Retest all fixed issues to verify the solution works (and didn’t break something else). Aim to have zero High severity issues open before pilot or launch.

**Pilot Test Plan:** Once QA is passed, conduct the pilot with real users as mentioned in the runbook: \- Recruit a small representative set of learners. \- Give them a timeframe to complete the course (and perhaps incentivize feedback). \- Observe their progress (maybe even sit with one or two if possible to see where they pause or have questions). \- After pilot, collect feedback via survey or interview focusing on: Were instructions clear? Any technical issues? Which content was confusing (if any)? Did they feel the activities helped them learn? \- Also analyze pilot data: e.g., pilot average quiz scores, questions frequently missed, time spent on each module (some LMS provide heatmaps or at least timestamps). \- Log any pilot-identified issues (content misunderstanding or technical) in the tracker too, and address before full rollout.

**Acceptance criteria for pilot:** If pilots can complete the course **without intervention**, and they achieve the intended learning outcomes (say average quiz score is acceptable, all can do the final assignment), and feedback is generally positive (or at least issues are minor and fixable), then the course is ready. If pilot shows a major design flaw (e.g., they all struggled with a section, indicating it was too hard or unclear), you may need a design revision and possibly a second pilot round.

**Go/No-Go Decision:** Use a checklist at end of QA/pilot: \- All critical test scenarios passed? **Yes** (if no, fix and re-test). \- SME and stakeholder approve content accuracy and tone? **Yes**. \- Accessibility audit passed? **Yes** (or exceptions documented with plan to fix ASAP). \- Pilot learners achieved outcomes and had minimal issues? **Yes**. \- If all yes, it’s a “Go” for launch. If any major no, iterate again.

Having this QA and pilot stage prevents costly failures at scale (e.g., catching that mis-set quiz rule with 1000 learners is much worse). It ensures you deliver a polished, reliable learning experience.

## I. 90-Day Rollout Plan

*How to use this:* This is a timeline for rolling out the eLearning program over roughly 90 days (a quarter). It includes phases, responsible roles, time estimates, and success metrics at each stage. Adapt the schedule to your project’s start date and specifics. It ensures a controlled deployment with time for feedback and iteration before scaling up.

**Phase 1: Preparation (Weeks 0–2)**  
\- **Key Activities:** Finalize content after pilot. Prepare support materials (FAQs, learner guide). Set up learner enrollment lists in the LMS. Stakeholder kickoff meeting to align on launch communications. \- **Roles:** Instructional Designer (final tweaks), LMS Admin (load users, set up course enrollment), Manager/Sponsor (craft endorsement message). \- **Success Metrics:** *Content readiness* (All pilot issues closed by week 1). *Stakeholder buy-in* (Sponsor approval obtained). *Technical readiness* (User accounts created, test emails sent successfully).

**Phase 2: Soft Launch Pilot (Weeks 3–4)**  
\- **Key Activities:** Conduct a small-scale launch (e.g., 5–10% of audience or one department). Monitor closely (daily check of LMS analytics, collect pilot user feedback surveys at end of week 4). Provide extra support to pilot users (e.g., office hours or rapid email responses). \- **Roles:** Pilot Participants (complete training), L\&D Analyst (monitor data), Support/Helpdesk (answer questions). \- **Success Metrics:** *Engagement:* ≥80% of pilot users complete the course within the 2-week pilot window. *Satisfaction:* Pilot user satisfaction survey average ≥ 4/5 for key questions (or qualitative feedback is majority positive). *No critical bugs:* Zero high-severity issues reported by pilot group.

**Phase 3: Analyze & Iterate (Week 5\)**  
\- **Key Activities:** Evaluate pilot results. Hold a debrief meeting with pilot users or collect their detailed feedback. Identify any content adjustments or technical fixes needed. Implement quick wins (small edits, clarify quiz wording, etc.) immediately during this week. If feedback suggests larger changes (e.g., need an additional example in Module 3), schedule those changes but don’t delay rollout unless it’s a showstopper. Update the course version (v1.1) if changes are made. \- **Roles:** Instructional Designer & SME (analyze feedback, make changes), Project Manager (decide on any scope changes/timeline shifts), LMS Admin (apply fixes in system). \- **Success Metrics:** *Issues resolved:* 100% of critical or common pilot issues addressed. *Green light to proceed:* Stakeholder sign-off that course is ready for wider rollout (or documented acceptance of any minor issues remaining).

**Phase 4: Full Launch (Weeks 6–8)**  
\- **Key Activities:** Open enrollment to all target learners (week 6). Announce launch via company-wide email or intranet (with a message from leadership highlighting importance and deadlines). If the training is mandatory, communicate the completion due date (e.g., end of week 8). Host a kickoff webinar or send an introductory video from the sponsor to build motivation. Monitor progress: weekly check-ins on enrollment and completion numbers. Send reminder emails at week 7 to those not started or in progress. Address support tickets promptly (technical issues or content questions). \- **Roles:** All Learners (self-paced completion), Communications team (assist in announcements), Sponsor/Managers (reinforce importance in team meetings). \- **Success Metrics:** *Participation:* 100% of target audience enrolled by end of week 6\. *Engagement:* At least 50% start the course within first week of launch. *Support:* All support queries answered within 24 hours. *Interim completion:* By end of week 8 (deadline), e.g. ≥90% completion rate if mandatory (or as defined goal).

**Phase 5: Reinforcement & Follow-up (Weeks 9–12)**  
\- **Key Activities:** For those who missed the initial deadline or are lagging, follow up individually. Managers get a report of any team members incomplete at week 9, so they can encourage completion. Perhaps offer a short live Q\&A session in week 9 for anyone who wants clarifications on content (this can help stragglers complete). Distribute a Level 1 feedback survey to all participants around week 10 to gather broader reaction data. Begin analyzing Level 2 data (learning results: quiz scores, etc.) to see if learning objectives met across the cohort. \- **Roles:** L\&D Analyst (compile reports, surveys), Managers (coach their teams), ID/SME (host Q\&A if needed). \- **Success Metrics:** *Completion:* By end of week 12, aim for 95–100% completion (for mandatory) or a specific target (for voluntary, e.g., 80% of target audience). *Learner feedback:* Survey response rate at least 50%; average satisfaction score ≥ 4/5. *Learning outcomes:* e.g., average final quiz score ≥ 80% or other measures showing competence gained (these should tie back to what “success” means for learning – if not met, plan remediation such as an optional refresher or adjusting content in future).

**Phase 6: Impact Assessment & Scale (Weeks 13–(Day 90))**  
\- **Key Activities:** After the training has been in effect \~90 days, measure **Level 3 and 4** results if possible. This means checking if on-the-job behavior or business metrics improved. For example, at week 13 and 90: ask managers via survey or interview, “Have you observed improvement in X?” or pull performance data (e.g., number of safety incidents pre vs post). Calculate ROI (Level 5\) if applicable by comparing any measured improvement to the cost. Document all findings. \- Also, if the program is ongoing (e.g., new hires will take it later), incorporate any necessary improvements before the next cohort. If scaling to other regions or languages, start localization process (see Localization notes in section 6). \- **Roles:** L\&D Analyst or Evaluator (data collection and analysis), Business Analyst (for pulling metrics), Sponsor (to review outcomes). Possibly Translators/Localizers if scaling globally after pilot region. \- **Success Metrics:** *Behavior change:* e.g., 75% of managers report noticeable improvement in skill X among their team. *Business impact:* e.g., KPI related to training shows positive change (target met, like 30% faster onboarding). *ROI:* If calculable, ROI \> 100% (meaning benefits in monetary terms outweigh cost – optional metric but nice to have[\[14\]](https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/#:~:text=One%20of%20the%20most%20frequently,ROI)). *Continuous improvement:* A documented plan (and budget if needed) for any iterative improvements or next steps by day 90\.

**Roles and Responsibilities Recap:** \- *Instructional Designer (ID):* Coordinates the design, updates, and ensures quality. Owns the blueprint and content integrity throughout. \- *Subject Matter Expert (SME):* Validates accuracy, provides content and examples, helps evaluate if learning transfer is happening (through quizzes or observed performance). \- *Project Manager (could be the ID in a dual role):* Tracks this timeline, ensures tasks happen on schedule, communicates between teams. \- *LMS Administrator:* Handles technical setup, user admin, data tracking, and resolves LMS-related issues. \- *Sponsor/Stakeholder:* Typically a senior manager who champions the program, communicates its importance, and looks for results. Also helps remove roadblocks (e.g., gets managers to give staff time to do the training). \- *Managers of Learners:* Reinforce completion, provide context of how training links to work, and observe behavior changes. \- *Learners:* Their “role” is to engage with content, provide feedback, and apply learning – not to be forgotten, sometimes offering a learner rep on the project team is valuable.

This 90-day plan ensures a **gradual rollout**: pilot first, then broad launch, then follow-up and measurement. It’s important not to consider day 1 of launch as the finish line – the real success is measured by day 90 and beyond, when we see if the training made a difference. Keep communicating wins back to stakeholders (e.g., “Already 500 employees certified in XYZ skill within first month\!”) to maintain support and momentum for learning initiatives. And any lessons learned in this rollout – document them for next time to continuously refine your playbook.

## J. Annotated Sources (Key References)

1. **NarayanKripa Sundararajan (2021) – *“4 Learning Science Strategies Proven To Boost Understanding,”* ISTE Blog.[\[2\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Retrieval%20practice%20is%20essentially%20the,you%20learn%20and%20remember%20it)[\[3\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Spaced%20practice)** – *Why it matters:* Summarizes evidence-based strategies like retrieval practice and spaced practice, citing 200+ experiments over 70 years showing that self-quizzing and spacing out learning vastly improve retention. Provides concrete, recent (2021) validation of these principles to justify their use in our playbook.

2. **Pooja K. Agarwal & Patrice M. Agostinelli (Spring 2020\) – *“Interleaving in Math,”* American Educator (AFT).[\[16\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20the%20example%20above%2C%20where,Even%20more%20dramatically%2C%20after%20one)[\[5\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20a%20recent%20study%2C%20nearly,term%20mathematics%20learning)** – *Why it matters:* Explains interleaving as a “desirable difficulty” with data from K–12 math studies – e.g., interleaved practice led to almost double retention after one month (74% vs 42% in one study). Though focused on math, the clear evidence from 2020 and discussion of desirable difficulties supports using interleaving and mixed practice in adult learning.

3. **Digital Learning Institute (n.d., \~2023) – *“Mayer’s 12 Principles of Multimedia Learning,”* DLI Blog.[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2)[\[29\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=What%20it%20means%3A%20Learning%20is,out%20the%20most%20critical%20elements)** – *Why it matters:* Concise rundown of Richard Mayer’s multimedia principles with practical tips (e.g. coherence principle: exclude extraneous material; signaling principle: highlight key info). Reinforces cognitive load management tactics. This modern summary ensures our content design recommendations (use visuals and audio wisely, avoid clutter) are rooted in well-researched guidelines.

4. **Jesse Fuhrman (2017) – *“Cognitive Load Theory: Helping Students’ Learning Systems Function More Efficiently,”* Franklin University I4 Blog.[\[39\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=can%20be%20used%20to%20install,learning%20as%20efficiently%20as%20possible)[\[27\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=,schema%2C%20particularly%20with%20novice%20leaners)** – *Why it matters:* Good primer on intrinsic, germane, and extraneous cognitive load and strategies to manage each. Cited tips like sequencing/chunking to handle intrinsic load and avoiding split attention/redundancy to cut extraneous load. Provides theoretical backbone (with references to Sweller) for our cognitive load and schema-building advice.

5. **Rustici Software (2018) – *“Kirkpatrick’s Training Evaluation Model,”* xAPI.com Blog.[\[12\]](https://xapi.com/kirkpatrick/#:~:text=1%20Reaction%20What%20did%20the,as%20a%20result%20of%20the)[\[36\]](https://xapi.com/kirkpatrick/#:~:text=xAPI%20makes%20it%20easier%20for,the%20learning%20experience%20and%20other)** – *Why it matters:* Straightforward explanation of Kirkpatrick’s 4 levels (Reaction, Learning, Behavior, Results) and how modern tech (xAPI) can help collect data especially for Levels 3 & 4\. This source (from the SCORM/xAPI experts) validates our inclusion of multi-level evaluation and suggests practical ways to gather evidence (like using mobile apps for on-job behavior capture).

6. **Watershed LRS (2019) – *“What’s Phillips’ Model for Learning Evaluation: ROI,”* Watershed Insights Blog.[\[14\]](https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/#:~:text=One%20of%20the%20most%20frequently,ROI)** – *Why it matters:* Introduces the Phillips Level 5 (ROI) in context of Kirkpatrick. We cite how it adds monetization of results – translating Level 4 impact to financial ROI. Important for demonstrating our playbook’s focus on business results and not just learning for learning’s sake. (Watershed are learning analytics pros, lending credibility to ROI focus).

7. **Timothy J. Cleary et al. (2023) – *“A Practical Review of Mastery Learning,”* American Journal of Pharmaceutical Education (PMC10159400).[\[7\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=Mastery%20learning%E2%80%99s%20goal%20is%20for,accomplish%20the%20predefined%20level%20of)[\[8\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=mastery%20are%20given%20corrective%20exercises,remediated%20until%20they%20demonstrate%20mastery)** – *Why it matters:* Describes how mastery learning ensures nearly all students achieve competency with corrective pathways. We took the concept of defining mastery, formative tests with feedback, and requiring 80–90% success before moving on. It’s a recent review (2023) in higher ed context, giving modern support to the classic Bloom mastery idea used in our design (remediation loops, etc.).

8. **Christie Wroten (2021) – *“Motivate Your Learners\! Self-Determination Theory for eLearning,”* eLearningIndustry.[\[10\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=1,when%20they%20feel%20supported%20to)[\[11\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=3,it%20motivates%20them%20to%20succeed)** – *Why it matters:* Summarizes how to satisfy the three SDT needs – competence, autonomy, relatedness – in online training. We used this to justify design elements like challenging-but-achievable tasks with feedback (competence), branching scenarios for choice (autonomy), and social learning/team activities (relatedness). It’s a practitioner-oriented piece reflecting psychological research in motivation.

9. **Dr. Nicole L’Etoile, Ed.D. (2025) – *“A Roadmap to WCAG 2.2 for Instructional Designers,”* L’Etoile Education Blog.[\[31\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=,drop%20or%20color)[\[33\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=2,A)** – *Why it matters:* Up-to-date (2025) guidance on implementing Web Content Accessibility Guidelines (WCAG 2.2) in eLearning. We cite specifics like captioning, keyboard navigation, and descriptive link text. This ensures our playbook’s accessibility recommendations are aligned with the latest standards (WCAG 2.2) and very practical for ID use.

10. **Baylor University ATL (2022) – *“Universal Design for Learning (UDL) Guidelines,”* Baylor Teaching Guide.[\[30\]](https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning#:~:text=UDL%20calls%20for%20multiple%20means,on%20these%20categories%20see%C2%A0CAST%20website)** – *Why it matters:* States clearly that UDL calls for multiple means of engagement, representation, and action/expression. This supported our emphasis on providing content in different formats and allowing multiple ways for learners to interact and show mastery. The source ties to CAST’s official framework, lending authority that our accessibility and variability strategies have an evidence-based origin.

11. **Brigham Young University Testing Center (2001) – *“14 Rules for Writing Multiple-Choice Questions,”* BYU Conf. Handbook.[\[17\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,solving%20and%20creativity)[\[21\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=answer%20correct%2013,Item%20Types%20Are%20More%20Appropriate)** – *Why it matters:* A classic set of item-writing rules, including avoiding “All of the above” and “None of the above,” with rationale. We referenced these rules to ensure our item-writing checklist aligns with long-established best practices that improve validity and reliability of exams. Even though older, these guidelines are still standard in testing literature and were validated by research (Haladyna et al.).

12. **Kritik (2021) – *“Designing Effective Rubrics for Peer Assessment,”* Kritik.io Blog.[\[22\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8DBest%20Practice%3A%20Levels)** – *Why it matters:* Cites Brookhart (2018) on optimal rubric levels and gives rubric design tips (like 3–5 performance levels to balance nuance vs. clarity). We used this to justify our 4-level rubric template and to emphasize clarity in descriptors. As rubrics are critical for consistent assessment, having recent backing on how to structure them improves our playbook’s recommendations for rubric quality.

*(Additional classic reference implicitly used: Wiggins & McTighe’s Understanding by Design for backward design and alignment – we referenced principles from it[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials) even if not separately cited by name.)*

These sources collectively cover **learning science fundamentals, design frameworks, assessment quality, motivation, accessibility, and evaluation** – the pillars of our playbook. Each was chosen for recency (many post-2018) or enduring authority, ensuring our guidance is both current and credible.

---

**(1) SME Gap List:** *(Areas where Subject Matter Experts may need guidance or input to avoid pitfalls in the design process)*  
\- **Avoiding info-dump:** SMEs often want to include every detail. IDs should remind SMEs of the learning **outcomes scope** – focusing on need-to-know content. Provide a template for SMEs to classify content as “Must know vs. Nice to know” to prevent overload.  
\- **Examples and stories:** SMEs might speak in abstract terms; ask them for concrete **real-world examples, case studies, or anecdotes**. Learners benefit from context – SMEs have those insider stories that make content relatable (but may not realize their importance).  
\- **Jargon clarification:** SMEs might use industry jargon or acronyms unconsciously. Have them define or simplify terms for novices. An **SME review for clarity** can catch jargon – or have them help create a glossary.  
\- **Assessment alignment:** SMEs may propose exam questions that test trivia or fringe cases they find interesting. The ID should work with SMEs to ensure questions directly align with learning objectives (perhaps by having SMEs map each question to an outcome in the blueprint). This also helps identify if the SME-written questions are at the right Bloom level (SMEs might inadvertently write overly complex questions).  
\- **Feedback on incorrect choices:** When SMEs review quiz content, get their insight on why learners might choose wrong answers. SMEs can help craft **feedback explanations** for misconceptions. This leverages SME knowledge of common mistakes in practice, turning it into teachable moments in feedback.  
\- **Maintenance commitment:** Secure SME agreement on maintaining the content. Often SMEs are busiest after go-live. Mitigation: schedule brief quarterly check-ins with SMEs to review content accuracy. Also ask SMEs during development: “What upcoming changes (legislation, new product releases, etc.) could affect this content?” and log those. This way SMEs are primed to update content and less likely to let it drift outdated.  
\- **Understanding learning design limits:** SMEs may expect certain complex skills to be learned solely via an eLearning. Communicate what the eLearning can do and where on-job training might still be needed. Essentially, align SME expectations: e.g., a one-hour module might teach principles, but **practice in the field** might be needed for mastery. SMEs appreciate this clarity so they don’t feel the training “failed” if some skills require more practice – set those as out-of-scope or part of post-training reinforcement.  
\- **SME Training in ID basics:** If possible, give SMEs a short primer on instructional design best practices (maybe share this playbook\!). An informed SME becomes a great ally who understands why we can’t just copy the 100-page manual into a course. This closes the gap between deep content knowledge and effective teaching strategy.

**(2) Minimal Example Course Map (Putting it All Together):**  
*To illustrate the full stack, here’s a miniature example of a course using our playbook, from outcomes to analytics:*

* **Course Title:** *Effective Team Communication* (Blended 4-week course for new managers)

* **Program Outcomes:** By end, managers will 1\) **Explain** core communication principles, 2\) **Demonstrate** active listening, 3\) **Resolve** a team conflict scenario, 4\) **Create** a personal improvement plan. *(These map to Knowledge, Skill, Ability, and Reflection competencies.)*

* **Module 1: Communication Basics** – *Outcome 1 (Explain principles).*

* *Content:* 15-min micro-lesson (video \+ bullet summaries) introducing 5 key principles (clarity, empathy, etc.). Hook with a quick bad/good communication example.

* *Activity:* Drag-and-drop matching exercise (match principle to example behavior) for practice.

* *Assessment:* 5-question quiz (MCQs) on principles. *Completion rule:* 80%+ required.

* *Analytics:* Track quiz success on first try; if \<80%, system flags and offers retake. We find 70% passed first try, others on second – indicates moderate grasp. Principle 3 question had 50% error rate – flagged to review content clarity on that principle in next update.

* **Module 2: Active Listening Skills** – *Outcome 2 (Demonstrate listening).*

* *Content:* 30-min lesson (could be live webinar or self-paced interactive) demonstrating listening techniques (body language, paraphrasing). Includes a role-play video example.

* *Practice:* Learners audio-record themselves in a 2-min mock conversation with a “difficult employee” (provided scenario) and upload. Also answer reflection questions on what they did.

* *Assessment:* Each learner’s recording is peer-reviewed by 2 others using a rubric (the one from section E example). Must score “Proficient” or above on all criteria to pass Outcome 2\. If not, they’re assigned an extra coaching call with instructor as remediation.

* *Analytics:* LMS records rubric scores and time spent. By analyzing, we see most struggled with “Reflective Responses” criterion (avg 2.5/4). Instructor notes this and in Week 3 sends an extra tip sheet on paraphrasing. Engagement: 95% did the recording – high participation indicating task was feasible.

* **Module 3: Conflict Resolution** – *Outcome 3 (Analyze/resolve conflict).*

* *Content:* 20-min case study reading (scenario of team conflict) \+ discussion forum. Learners discuss how they’d handle it (must post before seeing others).

* *Assessment:* Branching scenario quiz: Navigate a simulated conflict conversation with a virtual team member. There are optimal paths (de-escalate and resolve) vs. suboptimal (conflict worsens). Learners get scored on path (100, 50, or 0 points for outcome achieved). They can retry up to 2 times to get a better result, with feedback after each path (“You interrupted the employee, which escalated anger – try a different approach.”).

* *Completion rule:* Must achieve a successful resolution (score 100\) by second attempt.

* *Analytics:* We capture which choices were commonly selected. Notably, 40% initially chose an authoritarian response (which leads to conflict worsening). After feedback, 90% corrected in attempt 2\. This insight is shared with the training sponsor – it reveals a tendency in management style we might address further (e.g., add more content on collaborative conflict resolution in future). Discussion forum sentiment analysis showed common concern: “what if higher management doesn’t back me?” – facilitator addresses this concern in a bonus resource.

* **Module 4: Personal Improvement Plan** – *Outcome 4 (Create plan).*

* *Content:* Template and guidelines for a 3-month communication improvement plan (learners fill sections: goals, specific actions like “practice active listening in each team meeting,” metrics for success). Examples given from a fictional manager.

* *Activity/Assessment:* Learner submits their plan document. It’s not graded per se, but each gets qualitative feedback from the instructor. Also, they present key parts in a final live Zoom (optional) to reinforce commitment (peer pressure in a positive way).

* *Completion rule:* Submission required for course completion.

* *Analytics:* 100% submitted (due to manager support – managers were CC’d on completion status). Common goal in plans: “listen more, talk less” appeared in 80% of plans, showing training hit a chord. At 90-day follow-up, we check with learners if they did their actions – about 70% report improvement.

* **Wrap-Up & Evaluation:** Course ends with a feedback survey (Level 1\) – 4.5/5 avg rating on usefulness. Level 2 data: All passed required assessments. Level 3: Three months later, their teams report smoother meetings (subjective but backed by fewer escalation incidents recorded). Sponsor is pleased that the business sees fewer interpersonal conflicts; credits training as a factor.

This minimal example showcases alignment: each outcome was taught, practiced, assessed, and tracked. The governance is evident – we adjusted mid-course based on analytics (extra tip sheet). Accessibility was considered (captioned videos, alt text in case study infographic, etc.). The SME (an HR communications expert) provided the case study and reviewed all quiz branches to ensure realism. By following the playbook, this small course achieved its goals and provided data to prove it – a “rock-solid” success to model future programs on.

---

[\[1\]](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/#:~:text=A%20defining%20feature%20of%20Backward,learning%20activities%20and%20instructional%20materials)  Backward Design | Center for the Advancement of Teaching Excellence | University of Illinois Chicago 

[https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/](https://teaching.uic.edu/cate-teaching-guides/syllabus-course-design/backward-design/)

[\[2\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Retrieval%20practice%20is%20essentially%20the,you%20learn%20and%20remember%20it) [\[3\]](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding#:~:text=Spaced%20practice) ISTE | 4 Learning Science Strategies Proven To Boost Understanding | ISTE

[https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding](https://iste.org/blog/4-learning-science-strategies-proven-to-boost-understanding)

[\[4\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=2) [\[9\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=1) [\[28\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=4) [\[29\]](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning#:~:text=What%20it%20means%3A%20Learning%20is,out%20the%20most%20critical%20elements) Mayer's 12 Principles of Multimedia Learning | DLI

[https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning](https://www.digitallearninginstitute.com/blog/mayers-principles-multimedia-learning)

[\[5\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20a%20recent%20study%2C%20nearly,term%20mathematics%20learning) [\[6\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=Researchers%20refer%20to%20the%20benefits,and%20practice%20what%20they%20know) [\[16\]](https://www.aft.org/ae/spring2020/agarwal_agostinelli#:~:text=In%20the%20example%20above%2C%20where,Even%20more%20dramatically%2C%20after%20one) Interleaving in Math

[https://www.aft.org/ae/spring2020/agarwal\_agostinelli](https://www.aft.org/ae/spring2020/agarwal_agostinelli)

[\[7\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=Mastery%20learning%E2%80%99s%20goal%20is%20for,accomplish%20the%20predefined%20level%20of) [\[8\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=mastery%20are%20given%20corrective%20exercises,remediated%20until%20they%20demonstrate%20mastery) [\[15\]](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/#:~:text=knowledge,is%20used%20to%20check%20for)  A Practical Review of Mastery Learning \- PMC 

[https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/](https://pmc.ncbi.nlm.nih.gov/articles/PMC10159400/)

[\[10\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=1,when%20they%20feel%20supported%20to) [\[11\]](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning#:~:text=3,it%20motivates%20them%20to%20succeed) Motivate Your Learners\! The Self-Determination Theory for e-Learning \- eLearning Industry

[https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning](https://elearningindustry.com/motivate-learners-self-determination-theory-e-learning)

[\[12\]](https://xapi.com/kirkpatrick/#:~:text=1%20Reaction%20What%20did%20the,as%20a%20result%20of%20the) [\[13\]](https://xapi.com/kirkpatrick/#:~:text=4%20Results%20What%20was%20the,have%20ready%20access%20to%20it) [\[36\]](https://xapi.com/kirkpatrick/#:~:text=xAPI%20makes%20it%20easier%20for,the%20learning%20experience%20and%20other) Kirkpatrick training evaluation model

[https://xapi.com/kirkpatrick/](https://xapi.com/kirkpatrick/)

[\[14\]](https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/#:~:text=One%20of%20the%20most%20frequently,ROI) What’s Phillips Model for Learning Evaluation: A Quick Start Guide

[https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/](https://www.watershedlrs.com/blog/learning-evaluation/phillips-model/)

[\[17\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,solving%20and%20creativity) [\[18\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=%E2%80%A2%20Students%20merely%20need%20to,students%20know%20the%20correct%20answer) [\[19\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=11,%E2%80%9CNone%20of%20the%20Above%E2%80%9D%20Option) [\[20\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=12,%E2%80%9CNone%20of%20the%20Above%E2%80%9D%20Option) [\[21\]](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf#:~:text=answer%20correct%2013,Item%20Types%20Are%20More%20Appropriate) Microsoft Word \- Handouts \- 14 Rules for Writing Multiple-Choice Questions..

[https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf](https://testing.byu.edu/handbooks/14%20Rules%20for%20Writing%20Multiple-Choice%20Questions.pdf)

[\[22\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8DBest%20Practice%3A%20Levels) [\[23\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8D) [\[24\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=Typically%2C%20students%20find%20most%20rubrics,what%20is%20expected%20of%20them) [\[25\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=Best%20Practice%3A%20Criteria) [\[26\]](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2#:~:text=%E2%80%8D) Kritik \- Designing Effective Rubrics for Peer Assessment Methods

[https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2](https://www.kritik.io/blog-post/using-rubric-criteria-and-levels-to-ensure-accuracy-in-peer-assessment-2)

[\[27\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=,schema%2C%20particularly%20with%20novice%20leaners) [\[34\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=,suggested%20videos%20when%20it%20ends) [\[39\]](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently#:~:text=can%20be%20used%20to%20install,learning%20as%20efficiently%20as%20possible) Cognitive Load Theory: Helping Students' Learning Systems Function More Efficiently | Franklin University

[https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently](https://www.franklin.edu/institute/blog/cognitive-load-theory-helping-students-learning-systems-function-more-efficiently)

[\[30\]](https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning#:~:text=UDL%20calls%20for%20multiple%20means,on%20these%20categories%20see%C2%A0CAST%20website) Universal Design for Learning | Academy for Teaching and Learning | Baylor University

[https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning](https://atl.web.baylor.edu/teaching-guides/preparing-teach/universal-design-learning)

[\[31\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=,drop%20or%20color) [\[32\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=1,A) [\[33\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=2,A) [\[35\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=,drop%20or%20color) [\[37\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=1.1.1%20Non) [\[38\]](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers#:~:text=Step%201%3A%20Understand%20What%20WCAG,2%20Is%20All%20About) A Roadmap to WCAG 2.2 for Instructional Designers

[https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers](https://www.letoile-education.com/blog/a-roadmap-to-wcag-2-2-for-instructional-designers-and-e-learning-developers)