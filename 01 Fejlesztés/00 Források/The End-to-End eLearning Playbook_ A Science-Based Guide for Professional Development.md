

# **The End-to-End eLearning Playbook: A Science-Based Guide for Professional Development**

## **Executive Summary**

This playbook provides a comprehensive, evidence-based, and actionable framework for creating high-impact eLearning programs tailored for adult professional learners. Its purpose is to guide learning and development (L\&D) professionals, instructional designers, and program managers through the entire lifecycle of eLearning development, from initial strategic analysis to post-launch governance and continuous improvement. The core philosophy underpinning this guide is that effective learning is not merely the transmission of information but the intentional design of experiences that foster tangible skill acquisition, ensure durable long-term retention, and drive measurable on-the-job application.

The playbook is structured into four distinct parts. Part I establishes the foundational principles of learning science that govern how adults acquire and retain new competencies, focusing on cognitive load management, strategies for building durable knowledge, motivational drivers, and designing for accessibility. Part II presents a detailed, phased approach to the program lifecycle, integrating systematic project management frameworks like Backward Design and ADDIE with evidence-based instructional strategies. It covers strategic analysis, design and blueprinting, agile development, implementation, and robust evaluation. Part III addresses the operational and governance frameworks necessary for sustaining a successful eLearning program, including project management workflows, version control, and policies for course maintenance and archival. Finally, Part IV delivers a comprehensive toolkit of actionable deliverables—including templates, blueprints, checklists, and runbooks—designed for immediate adaptation and use.

By systematically integrating the science of how people learn with the discipline of how projects are managed, organizations can elevate their training initiatives. The key takeaway is that this integrated approach enables a strategic shift from "check-the-box" compliance training to the creation of dynamic learning ecosystems that demonstrably enhance employee capability and drive tangible business results.

---

## **Part I: The Foundation: Principles of Effective Adult Learning**

### **Chapter 1: Designing for the Professional Brain: Core Principles of Learning Science**

The development of effective eLearning is fundamentally an exercise in applied cognitive science. Before any module is built or content is written, it is imperative to understand the mechanisms by which adult professionals learn, process, and retain information. This chapter establishes the scientific bedrock of the entire playbook, arguing that an understanding of cognitive architecture is a non-negotiable prerequisite for designing instruction that works. The central challenge in professional development is to facilitate the transformation of a learner from a novice, who possesses fragmented information, into an expert, who operates from a rich and interconnected set of mental models, or schemas.1 This transformation is not accidental; it is the result of carefully designed learning experiences that respect the brain's inherent capabilities and limitations.

#### **1.1. Managing Cognitive Load: The Architecture of Learning**

The human brain's capacity to process new information is finite. Cognitive Load Theory (CLT), pioneered by John Sweller, posits that our working memory—the cognitive system responsible for temporarily holding and manipulating information—is severely limited and can only handle a few pieces of novel data at once.1 When this limited capacity is overwhelmed, learning is disrupted. Effective instruction, therefore, is a process of managing this "mental energy" to direct the learner's cognitive resources toward activities that are most relevant to learning.1 CLT categorizes cognitive load into three types:

* **Intrinsic Load:** The inherent difficulty and complexity of the subject matter itself. This is determined by the number of interacting elements a learner must process simultaneously.2  
* **Extraneous Load:** The mental effort required to process information that is not directly relevant to the learning objective. This is generated by the way information is presented, such as confusing layouts, distracting visuals, or redundant text.2  
* **Germane Load:** The mental effort dedicated to processing information, constructing new schemas, and integrating them into long-term memory. This is the "good" cognitive load that leads to deep learning.2

The primary goal of instructional design is to reduce extraneous load and manage intrinsic load, thereby freeing up cognitive resources to be dedicated to germane load.2 Practical application of this principle involves several key tactics. To reduce extraneous load, designers must eliminate non-essential "nice-to-know" information, ensuring every element serves the learning objective.1 They should also physically integrate related sources of information, such as placing text labels directly onto a diagram rather than in a separate legend, to avoid the "split-attention effect" that forces learners to expend mental energy connecting disparate elements.1 Furthermore, redundant information—such as narrating on-screen text verbatim—should be avoided, as processing the same information in two modalities at once adds unnecessary cognitive strain.1

To manage intrinsic load, especially for complex topics, content should be broken down into smaller, more manageable segments, a technique known as chunking.4 This allows learners to process and master individual components before integrating them into a larger whole. Instructional scaffolding, where support is gradually faded as the learner gains proficiency, is another critical technique for managing the inherent difficulty of a topic.2

Maximizing germane load involves directing the learner's attention toward schema acquisition. For novices, one of the most powerful techniques is the use of **worked examples**. These step-by-step demonstrations of how to solve a problem or complete a task reduce the cognitive burden of "discovery" and allow learners to focus their mental energy on understanding the underlying process and principles.1

The principles of cognitive load management are not static; they must adapt as a learner's expertise develops. Research clearly distinguishes between the cognitive processes of novices and experts, noting that instructional strategies highly effective for one group can be ineffective or even detrimental for the other.1 For instance, worked examples are most beneficial for novices who lack the requisite schemas to solve problems independently.7 However, as learners gain expertise, these same worked examples can become redundant and impose an extraneous cognitive load, a phenomenon known as the "expertise reversal effect".7

This dynamic relationship between expertise and cognitive load means that a one-size-fits-all eLearning course designed for a mixed-expertise audience is inherently suboptimal. An effective program must be designed with adaptive pathways or tiered content. A practical implementation would involve a program that begins with highly scaffolded modules rich with worked examples, suitable for all learners. As individuals demonstrate proficiency through assessments, they could unlock "expert paths" that systematically fade the instructional support, transitioning from worked examples to partially completed problems and finally to independent problem-solving challenges. This approach ensures that cognitive load is managed appropriately across the entire learning journey, challenging learners optimally at each stage of their development from novice to expert.

#### **1.2. Building Durable Knowledge: The Power of Effortful Retrieval**

For learning to have a lasting impact, knowledge must be encoded into long-term memory in a way that makes it durable and accessible for future use. Decades of cognitive science research have demonstrated that some of the most common study habits, such as passive re-reading and cramming (massed practice), are among the least effective for long-term retention.8 Instead, more effortful and active strategies produce far more robust and durable learning.

Three of the most powerful evidence-based strategies are:

* **Spaced Practice:** Learning is more effective when it is distributed over time rather than crammed into a single session. Spacing learning allows for some forgetting to occur, which makes the subsequent act of recalling the information more effortful and thus strengthens the memory trace.8 In an eLearning context, this means designing programs that deliver content in shorter modules spread across several days or weeks, rather than a single, multi-hour course. Automated reminders via email or the Learning Management System (LMS) can prompt learners to return for the next spaced session.  
* **Retrieval Practice:** The act of actively recalling information from memory is a powerful learning event in itself, often referred to as the "testing effect".8 Each time information is successfully retrieved, its memory trace is strengthened and becomes more easily accessible in the future. This strategy consistently outperforms passive review.12 Practical eLearning applications include embedding frequent, low-stakes quizzes at the beginning or end of modules, using interactive flashcard activities, or incorporating "brain dump" exercises where learners are prompted to write down everything they can remember about a topic without consulting their notes.8 It is critical that these activities are framed as practice for learning, not as formal, high-stakes assessments, to reduce anxiety and encourage participation.8  
* **Interleaving:** Instead of learning related concepts in blocked succession (e.g., studying all of topic A, then all of topic B), interleaving involves mixing the practice of these related concepts. This forces the learner to discriminate between different types of problems and select the appropriate solution strategy, which leads to better long-term retention and improved ability to transfer learning to new contexts.13 For example, a sales training module on handling objections could interleave practice scenarios for price objections with scenarios for product-feature objections, rather than grouping them by type.

These powerful learning strategies are all manifestations of a broader principle known as "desirable difficulties".10 This concept, introduced by Robert Bjork, posits that learning activities that introduce a considerable but desirable amount of effort lead to more robust and long-lasting learning.10 The challenge inherent in retrieving spaced information or distinguishing between interleaved problems makes the learning process feel harder in the short term, but it is this very effort that strengthens long-term memory.

This presents a significant implementation challenge. Learners often mistake the fluency of passive review for effective learning and can perceive effortful strategies as less effective because they feel more difficult.15 This mismatch between the perception and reality of learning effectiveness means that simply designing a course with these principles is insufficient. It must be accompanied by a learner and stakeholder education strategy. The playbook must therefore advise L\&D professionals to be transparent about the instructional design. This involves explicitly communicating to learners

*why* the course is designed to be challenging, explaining that the cognitive effort they are expending is a direct indicator of effective, durable learning. By setting these expectations upfront, designers can manage learner frustration, increase buy-in for more rigorous learning activities, and ultimately deliver a more impactful educational experience.

#### **1.3. Fueling the Engine: Motivation and Engagement**

While cognitive principles dictate *how* to structure information for optimal processing, motivational principles determine *whether* a learner will engage with the material in the first place. For adult professionals, who are often balancing learning with demanding job responsibilities, motivation is a critical factor. Self-Determination Theory (SDT) provides a robust framework for understanding and fostering motivation. SDT posits that all humans have three innate psychological needs that, when satisfied, lead to high-quality, self-determined motivation and engagement.17 These needs are:

* **Autonomy:** The need to feel a sense of choice, control, and volition over one's actions. Learners who feel autonomous perceive that they are engaging in tasks freely, without coercion.17  
* **Competence:** The need to feel effective and capable in one's interactions with the environment. Learners who feel competent are confident in their ability to meet challenges and achieve mastery.17  
* **Relatedness:** The need to feel connected to and cared for by others. In a learning context, this involves feeling a sense of belonging within a community of peers and instructors.17

Instructional design can directly support these needs. To foster **autonomy**, eLearning programs can provide learners with meaningful choices, such as allowing them to select the order in which they complete non-sequential modules, choose between different formats for a final project (e.g., a written case study analysis, a video presentation, or a portfolio of work), or explore optional "deep-dive" resources based on their interests.18

To build a sense of **competence**, the learning journey should be structured to enable mastery. This can be achieved through a mastery learning approach, where learners must demonstrate a high level of proficiency (e.g., 90% on an assessment) on one topic before advancing to the next, with ample opportunities for practice, feedback, and remediation.19 Providing clear, transparent rubrics for assignments and delivering targeted, actionable feedback are also essential for helping learners understand the path to success and feel a sense of progress.20

To cultivate **relatedness**, designers can incorporate social learning elements even in asynchronous eLearning. This can include structured peer feedback activities in discussion forums, collaborative projects using shared documents or virtual whiteboards, or organizing learners into cohorts that progress through the program together, fostering a sense of shared experience and mutual support.

While all three needs are important for holistic well-being, their relative impact on academic motivation can differ. A comprehensive meta-analysis of SDT in educational settings revealed that the satisfaction of the need for competence was the strongest and most consistent predictor of self-determined motivation.17 For adult professional learners, this finding is particularly salient. The primary driver for undertaking workplace training is often the desire to become more effective and capable in their professional roles. This creates a powerful causal chain: a program that successfully builds a tangible sense of competence will foster higher-quality motivation, which in turn leads to greater persistence, deeper engagement, and ultimately, higher achievement. This creates a virtuous cycle of success. Conversely, a program that fails to instill a credible sense of competence in its learners is likely to result in amotivation and disengagement, regardless of how many choices (autonomy) or social features (relatedness) it offers.

This places the development of competence at the very center of effective eLearning design for professionals. It elevates practices like authentic, performance-based assessment and mastery-based progression from being merely "good ideas" to being the fundamental pillars upon which the entire learning experience must be built. The ultimate goal of a professional development program is not for the learner to simply "complete" the training, but for them to emerge with a justified and confident belief in their newly enhanced capabilities, ready to apply them in their work.

##### **Table 1.1: Learning Science Principles in Practice**

| Learning Science Principle | Core Idea | eLearning Implementation Tactic |
| :---- | :---- | :---- |
| **Cognitive Load Theory (CLT)** | Working memory is limited; avoid overwhelming it with extraneous processing. 1 | Use a narrated animation to explain a complex process, avoiding redundant on-screen text that mirrors the narration. 3 |
| **Dual Coding** | Information is processed more effectively when presented in both verbal and visual formats. 4 | Accompany a textual explanation of a four-step process with a simple, clear infographic that visually represents the same four steps. 4 |
| **Spaced Practice** | Distributing learning over time leads to better long-term retention than cramming. 8 | Design a five-module course to be delivered over five days, with each module unlocking 24 hours after the previous one was completed. |
| **Retrieval Practice** | Actively recalling information from memory strengthens that memory. 9 | Begin each module with a brief, low-stakes, multiple-choice quiz that covers key concepts from the preceding module. 8 |
| **Interleaving** | Mixing the practice of related but distinct skills improves the ability to differentiate between them. 13 | In a compliance course, create a scenario-based quiz that mixes questions about data privacy with questions about information security, rather than grouping them into separate sections..14 |
| **Desirable Difficulties** | Learning activities that require considerable but achievable effort lead to more durable learning. 14 | Instead of a simple definition matching exercise, use a short-answer question that requires the learner to explain a concept in their own words. 14 |
| **Self-Determination Theory (SDT)** | Motivation is driven by the needs for Autonomy, Competence, and Relatedness. 17 | For a final capstone project, allow learners to choose between analyzing a provided case study or analyzing a real-world problem from their own work context. 23 |
| **Mastery Learning** | Learners must achieve a high level of proficiency in one topic before moving to the next. 19 | Gate progression to the next module until the learner achieves a score of at least 90% on the end-of-module assessment, allowing for multiple retakes. 19 |

### **Chapter 2: The Inclusive Digital Classroom: Designing for Accessibility**

Creating an effective learning program requires designing for the full spectrum of human variability. Accessibility in eLearning is not a peripheral compliance issue or an add-on feature; it is a fundamental component of instructional quality that benefits all learners. By proactively designing to remove barriers, we create more robust, flexible, and usable experiences for everyone. This chapter introduces two complementary frameworks that provide the strategic and tactical foundations for building truly inclusive digital learning: Universal Design for Learning (UDL) and the Web Content Accessibility Guidelines (WCAG).

#### **2.1. Universal Design for Learning (UDL): Proactive Flexibility**

Universal Design for Learning is a pedagogical framework based on scientific insights into how humans learn. Its core principle is to improve and optimize learning for all people by proactively designing flexible learning environments with options to reduce barriers.18 Rather than creating a single, one-size-fits-all pathway and then providing accommodations for those who cannot access it, UDL encourages the design of multiple, flexible pathways from the outset. The framework is organized around three primary principles that align with the brain's recognition, strategic, and affective networks 24:

1. **Provide Multiple Means of Engagement (the "Why" of Learning):** This principle focuses on tapping into learners' interests, offering appropriate challenges, and increasing motivation. To apply this, designers can optimize individual choice and autonomy by allowing learners to set their own goals or select the context for a task. They can enhance relevance and authenticity by connecting content to real-world problems that are meaningful to the learners.18 Creating a supportive and accepting learning climate that minimizes threats and distractions is also crucial for sustaining engagement.18  
2. **Provide Multiple Means of Representation (the "What" of Learning):** This principle addresses the fact that learners perceive and comprehend information differently. To implement this, information must be presented in multiple formats. For example, a core concept could be explained through text, an explanatory video with captions, an audio-only podcast with a transcript, and a visual diagram with descriptive alt-text.23 This allows learners to access the information through the modality that works best for them. It also involves clarifying vocabulary, syntax, and symbols, and activating or supplying background knowledge to ensure a shared foundation for understanding.18  
3. **Provide Multiple Means of Action & Expression (the "How" of Learning):** This principle recognizes that learners vary in how they navigate a learning environment and express what they know. To support this, designers should offer multiple ways for learners to demonstrate their knowledge. Instead of relying solely on a traditional written essay, options could include creating a video presentation, building a portfolio, recording a podcast, or designing an infographic.23 This principle also involves providing scaffolds, models, and feedback to support learners in developing their executive functions, such as goal-setting and strategic planning.18

#### **2.2. WCAG 2.2 AA: The Technical Standard**

While UDL provides the pedagogical strategy for inclusive design, the Web Content Accessibility Guidelines (WCAG) provide the technical standard for ensuring digital content can be accessed by people with a wide range of disabilities. Developed by the World Wide Web Consortium (W3C), WCAG is the global benchmark for web accessibility.25 The guidelines are organized under four foundational principles, known by the acronym POUR:

1. **Perceivable:** Information and user interface components must be presentable to users in ways they can perceive. This means providing text alternatives for non-text content, captions for audio, and ensuring content is visually distinguishable with sufficient color contrast.25  
2. **Operable:** User interface components and navigation must be operable. This requires that all functionality is available from a keyboard, users have enough time to read and use content, and content does not cause seizures or physical reactions.25  
3. **Understandable:** Information and the operation of the user interface must be understandable. This involves making text readable and understandable, making web pages appear and operate in predictable ways, and helping users avoid and correct mistakes.25  
4. **Robust:** Content must be robust enough that it can be interpreted reliably by a wide variety of user agents, including assistive technologies. This means using standard-compliant markup and ensuring components have a name, role, and value that can be programmatically determined.25

WCAG defines three levels of conformance: A (lowest), AA, and AAA (highest). Level AA is the most commonly cited standard in international legislation and corporate policies, representing a level of accessibility that is robust and achievable for most digital content.25 Key WCAG 2.2 Level AA criteria relevant to eLearning include providing captions for live and prerecorded video 25, ensuring a minimum color contrast ratio of 4.5:1 for normal text 25, making sure text can be resized up to 200% without loss of content, and ensuring all interactive elements have a visible focus indicator for keyboard users.25

The relationship between UDL and WCAG is complementary and essential for creating truly accessible learning. UDL provides the overarching pedagogical *strategy* for designing inclusive learning experiences by focusing on flexibility, choice, and proactive barrier removal. WCAG, in turn, provides the specific, testable technical *tactics* required to execute that strategy in a digital medium. One framework informs the "what" and "why" of inclusive design, while the other governs the "how" of technical implementation.

A program can be technically WCAG compliant yet remain pedagogically rigid and inaccessible to learners with cognitive differences if it fails to incorporate UDL principles. For instance, a course consisting solely of long, text-heavy PDFs might meet WCAG standards but would present significant barriers to a learner with dyslexia or ADHD. Conversely, a program that embraces UDL by offering a choice between reading an article or watching a video might fail if the video is not WCAG compliant because it lacks captions. True accessibility requires a dual focus: a strategic commitment to pedagogical flexibility (UDL) combined with rigorous adherence to technical standards (WCAG). This integrated approach prevents the common pitfall of focusing solely on technical compliance while neglecting the diverse ways in which people actually learn.

---

## **Part II: The Program Lifecycle: A Phased Approach to Development**

### **Chapter 3: Phase 1 \- Strategic Analysis & Scoping**

The success or failure of an eLearning program is most often determined in its initial phase. Before a single slide is designed or line of code is written, a rigorous process of analysis and scoping must be undertaken to ensure the program is aimed at the right problem, for the right audience, with the right goals. A failure at this stage guarantees a program that is, at best, a waste of resources and, at worst, a source of frustration that undermines the credibility of the L\&D function. This phase is about defining the problem with precision before attempting to build the solution. To provide a robust structure for this critical work, this playbook adopts the **Backward Design** framework, a three-step process that begins with the end in mind.26

#### **3.1. Step 1 of Backward Design: Identify Desired Results**

The foundational principle of Backward Design is to start by identifying the desired results of the learning intervention.26 This approach intentionally defers any discussion of content, activities, or technology until a clear and shared understanding of the ultimate goals has been established. This process involves moving from broad business needs to specific, measurable learner outcomes.

The first step is to conduct a **Training Needs Analysis (TNA)** to identify and validate the performance gap that the training is intended to address. This involves gathering data to understand the difference between the current state of performance and the desired state. Methods can include surveys, interviews with managers and high-performers, direct observation, and analysis of business data (e.g., sales figures, customer satisfaction scores, error rates).28 The output of the TNA is a clear problem statement.

From this problem statement, the team must work with business stakeholders to define the desired **Business Outcomes**. These are the high-level, tangible impacts the program is expected to have on the organization. A well-defined business outcome is specific and measurable, such as "Reduce customer support ticket resolution time by 15% within six months of program completion" or "Increase the close rate for new sales leads by 10% in the next fiscal quarter".29

With the business outcome defined, the next step is to create a **Competency Framework or Map**. This process breaks down the high-level business goal into the specific knowledge, skills, and behaviors (competencies) an employee needs to possess to achieve that outcome.30 Competency mapping involves identifying the core, common, and job-specific competencies required; defining each competency with clear behavioral indicators; and establishing proficiency levels (e.g., novice, intermediate, expert).32 This map provides the architectural foundation for the entire curriculum, ensuring that all subsequent learning content is directly tied to a required on-the-job capability.

Finally, the competencies are translated into **Measurable Learning Objectives**. These statements describe precisely what a learner will be able to do upon completion of the learning program. An effective learning objective is constructed using the Condition-Behavior-Criterion (CBC) model:

* **Condition:** The circumstances under which the performance will occur (e.g., "Given a customer case study...").  
* **Behavior:** The observable action the learner will perform. This must be an active, measurable verb drawn from a framework like Bloom's Revised Taxonomy (e.g., "the learner will *analyze*," "the learner will *construct*," "the learner will *evaluate*").34 Ambiguous verbs like "understand" or "know" must be avoided.27  
* **Criterion:** The standard or level of acceptable performance (e.g., "...according to the four-step company service model.").

#### **3.2. Step 2 of Backward Design: Determine Acceptable Evidence**

Once the desired results are clearly defined, the second step of Backward Design is to determine what will count as acceptable evidence that those results have been achieved.26 This means designing the final, summative assessments

*before* planning any learning activities. This critical step ensures that the instructional content to be developed will be purposefully focused on preparing learners for the assessments, rather than being a collection of interesting but potentially irrelevant information.

The **Assessment Strategy** should employ a mix of formative and summative assessments. **Formative assessments** are low-stakes activities conducted *during* the learning process to monitor progress and provide feedback (e.g., knowledge checks, practice simulations, draft submissions).20 Their purpose is to support learning.

**Summative assessments** are higher-stakes evaluations conducted *at the end* of a unit or course to measure proficiency against the learning objectives (e.g., final exams, capstone projects, certification tests).38 Their purpose is to validate learning.

For adult professional learners, the assessment strategy should prioritize **Authentic Assessment**. Authentic assessments require learners to apply their knowledge and skills to perform complex tasks that mirror real-world challenges they would face in their jobs.39 Instead of a multiple-choice test on company policies, an authentic assessment might be a branching scenario simulation where the learner must navigate a difficult client conversation and make decisions based on those policies. Authentic tasks are more engaging, promote deeper learning, and facilitate the transfer of skills from the training environment to the workplace.40

To ensure the summative assessment is comprehensive and aligned, an **Assessment Blueprint** should be created. This is a matrix or table that maps each learning objective to specific assessment items and indicates the cognitive level being assessed (per Bloom's Taxonomy).41 This tool guarantees that all objectives are assessed, that the assessment reflects the relative importance of different topics, and that it measures not just recall of information but also higher-order thinking skills like analysis and evaluation.42

The creation of an assessment blueprint during the initial analysis phase serves a powerful strategic function that extends beyond simple instructional alignment. By forcing a concrete, detailed discussion with stakeholders and SMEs about what constitutes "acceptable evidence" of competence, the blueprint transforms the conversation from abstract desires—such as "we need our team to be more strategic"—to specific, observable, and measurable behaviors—for example, "a competent learner must be able to produce a project plan that correctly identifies critical path dependencies and includes a risk mitigation strategy."

This document becomes a powerful tool for communication and negotiation. It establishes a shared, explicit understanding of what success looks like. Consequently, the assessment blueprint functions as the definitive scope document for the project. Any content or activity proposed for inclusion in the course must directly support a learner's ability to perform successfully on a specific item within the blueprint. If a piece of content does not serve this purpose, it is, by definition, extraneous. According to Cognitive Load Theory, such extraneous material hinders learning and should be excluded.1 This evidence-based rationale allows the instructional design team to manage scope creep and push back against SME requests to "just add one more thing," positioning them as strategic partners focused on performance outcomes, not simply as content assemblers.

### **Chapter 4: Phase 2 \- Evidence-Based Design & Blueprinting**

With the desired results and assessment evidence clearly defined, the third and final step of Backward Design is to plan the learning experience and instructional activities.26 This is the phase where the principles of learning science from Part I are systematically woven into a coherent instructional sequence designed to equip learners with the knowledge and skills needed to succeed on the assessments. This process moves from a high-level program structure down to the screen-by-screen details of each learning asset.

#### **4.1. Structuring the Learning Journey: From Macro to Micro**

Effective instructional design requires a multi-layered approach to structure, ensuring coherence at the program, module, and content levels.

At the **course level**, structuring the program with a **narrative arc** can significantly enhance learner engagement and retention by providing a cohesive story that connects disparate lessons into a meaningful whole.43 This involves establishing a central theme or challenge at the beginning, building skills and knowledge through successive modules, and culminating in a capstone project or final assessment that resolves the initial challenge. Overarching instructional frameworks can provide a reliable structure for this journey.

**Gagné's 9 Events of Instruction**, for example, offers a systematic, step-by-step sequence for structuring a learning experience, from gaining attention to enhancing retention and transfer.44 Alternatively,

**Merrill's First Principles of Instruction** provides a problem-centered framework that cycles through activating prior knowledge, demonstrating new skills, applying those skills, and integrating them into the learner's real-world context.46

At the **module level**, establishing a **repeatable lesson pattern** or "module roadmap" creates a predictable and consistent experience for the learner, reducing extraneous cognitive load by making the structure familiar.48 A standard module template should incorporate key events from a framework like Gagné's, typically including: an introduction to gain attention and state objectives; a content presentation section; interactive practice activities to elicit performance and provide feedback; and a concluding assessment to measure mastery of the module's objectives.45

At the **content level**, the principles of **sequencing and chunking** are paramount. Content must be sequenced logically, building from simple to complex concepts and ensuring that prerequisite knowledge is established before more advanced topics are introduced.6 To manage cognitive load, this sequenced content should be broken down into small, focused "chunks," often in the form of microlearning modules that are typically 2-7 minutes in length.5 Each chunk should focus on a single concept or learning objective, allowing learners to process and consolidate information before moving on.

#### **4.2. The Design Document & Storyboard**

The outputs of the design phase are two critical documents that guide the entire development process: the Course Design Blueprint and the Storyboard.

The **Course Design Blueprint** (also known as a design document) is the master plan for the entire program. It is a comprehensive document that synthesizes all the outputs from the analysis and design phases into a single source of truth. A robust blueprint template should include sections for: Basic Project Information (title, owner, dates); Audience Analysis Summary; Business Goals and Program Rationale; a complete list of Measurable Learning Objectives; the detailed Assessment Strategy, including the Assessment Blueprint; a high-level Course Map illustrating the sequence of modules and lessons; Technical and Delivery Specifications (e.g., authoring tool, LMS); and a high-level Project Milestone timeline.52

The **Storyboard** is the detailed, screen-by-screen specification that translates the design blueprint into a concrete plan for development. It serves as the primary communication tool between the instructional designer, SMEs, and media developers. Best practices for storyboarding include using a standardized template, applying a "Goldilocks" approach to detail (enough information for the developer without being overwhelming), and including a visual map for complex branching scenarios.56 A versatile storyboard template typically uses a table format with columns for: Screen/Slide Number, On-Screen Text, Visual Elements (description of graphics, animations, or video), Audio/Narration Script, and Developer Notes (programming logic, interactivity, accessibility notes).57

The design blueprint and the storyboard serve distinct but complementary roles in project management and stakeholder communication. The Course Design Blueprint should be treated as a **strategic contract**. It is a high-level document that secures formal agreement and sign-off from executive sponsors and key business stakeholders on the *what* and *why* of the project—its goals, target audience, and how success will be measured. No detailed content development should commence until this strategic alignment is achieved and documented.

In contrast, the Storyboard is a **tactical architectural drawing**. It provides the granular, step-by-step instructions needed by the development team to build the actual learning assets. While essential for the project team and SMEs reviewing content for accuracy, presenting a 100-page storyboard to a senior leader is an inefficient use of their time and can derail strategic conversations with excessive detail. Therefore, a two-stage review process is recommended. First, the high-level Course Design Blueprint is reviewed and approved by executive stakeholders. Only after this strategic sign-off does the team proceed to develop the detailed storyboard, which is then reviewed and approved by SMEs and the direct project team. This tiered approach ensures the right people are reviewing the right level of detail at the right time, streamlining the approval process and maintaining strategic focus.

### **Chapter 5: Phase 3 \- Agile Development & Content Creation**

The development phase is where the detailed plans articulated in the storyboard are transformed into functional and engaging eLearning assets. This "build" phase requires a blend of creative media production and rigorous adherence to learning science principles. An agile, iterative approach is highly recommended, allowing for rapid prototyping and frequent feedback loops to ensure the final product aligns with the design vision and meets learner needs.

#### **5.1. Applying Multimedia Learning Principles**

The creation of multimedia content must be guided by evidence-based principles to ensure it facilitates learning rather than hindering it. Richard Mayer's 12 Principles of Multimedia Learning, which are strongly rooted in Cognitive Load Theory and Dual Coding Theory, provide a practical framework for this work.3 The central idea is to effectively leverage both the visual and auditory channels of working memory without overloading either one. A practical checklist for reviewing all multimedia assets should include:

* **Coherence Principle:** Is all content—including words, pictures, and sounds—essential to the learning goal? Extraneous or purely decorative elements should be removed as they can distract the learner and increase extraneous cognitive load.3  
* **Modality & Redundancy Principles:** Is information presented using graphics and narration rather than graphics, narration, and redundant on-screen text? When explaining a visual, it is more effective to use the auditory channel (narration) than to add on-screen text, which competes for limited visual processing capacity.3  
* **Contiguity Principles (Spatial & Temporal):** Are corresponding words and pictures presented near each other on the screen (spatial) and at the same time (temporal)? Aligning related elements in space and time reduces the cognitive effort required for learners to make connections.3  
* **Personalization & Voice Principles:** Is the narration delivered in a conversational, human-like style rather than a formal, machine-like tone? A more personal and friendly tone has been shown to promote deeper learning by fostering a sense of social partnership.58

#### **5.2. Designing Engaging Interactions**

Effective learning is an active process, not a passive one. To move beyond simple information presentation, eLearning must incorporate meaningful interactions that require learners to engage with the content. The type of interaction should be carefully matched to the learning objective.

* **Knowledge Checks:** For objectives focused on recall and comprehension, simple interactions like multiple-choice questions, drag-and-drop matching exercises, or flashcards can be used to facilitate retrieval practice.  
* **Simulations and Branching Scenarios:** For objectives focused on decision-making and procedural skills, branching scenarios provide a powerful tool for practice in a safe, consequence-free environment. The design process involves identifying the key decision points in a real-world task, creating a realistic scenario, and mapping out the various paths a learner can take based on their choices. Each branch should lead to a plausible consequence and provide targeted feedback that explains *why* a particular choice was effective or ineffective.59  
* **Deliberate Practice:** For developing true expertise, interactions should be designed according to the principles of deliberate practice. This involves creating tasks that are challenging but achievable (within the learner's "zone of proximal development"), providing opportunities for repetition and refinement, and offering immediate, specific feedback from a coach or expert system.61

#### **5.3. Managing the SME Collaboration Workflow**

The relationship between the Instructional Designer (ID) and the Subject Matter Expert (SME) is one of the most critical factors in the success of an eLearning project. The SME provides the raw content and accuracy, while the ID provides the expertise in learning science and instructional methodology. A structured and well-managed collaboration workflow is essential to leverage the strengths of both roles effectively.

* **Project Kickoff:** The collaboration should begin with a formal kickoff meeting that establishes clear roles, responsibilities, communication protocols, and a detailed project timeline with specific deadlines for SME reviews.63  
* **Content Gathering:** To avoid the common problem of SMEs providing an unorganized "data dump" of everything they know, the ID should provide structured tools for content gathering. This could include content outline templates, question prompts, or structured interview guides that are aligned with the learning objectives.  
* **Iterative Review Cycles:** The development process should be broken into distinct review cycles (e.g., design document review, storyboard review, alpha build review, beta build review). Using a centralized, visual feedback tool can streamline this process, allowing SMEs to leave comments directly on the content and preventing the confusion that arises from conflicting feedback in long email chains.63  
* **Defining Roles:** It is crucial to reinforce the distinct expertise of each role throughout the project. The SME is the expert on the *content*, responsible for its accuracy and completeness. The ID is the expert on the *learning experience*, responsible for translating that content into a format that is instructionally sound. This means the ID must be empowered to make design decisions based on learning principles, including pushing back respectfully on SME requests that would violate those principles, such as adding extraneous information or using ineffective presentation methods.

### **Chapter 6: Phase 4 \- Implementation & Rollout**

A brilliantly designed and developed eLearning program can still fail if its launch is poorly executed. The implementation and rollout phase is a critical project in its own right, requiring careful planning and execution to ensure a smooth technical deployment, a positive learner reception, and strong organizational adoption. This chapter provides a systematic process for testing, deploying, and communicating the new program to maximize its impact.

#### **6.1. Quality Assurance (QA) & Testing**

Before any learner sees the course, it must undergo a rigorous quality assurance process to identify and eliminate any errors. A professional, polished final product builds learner trust and prevents distractions that can create extraneous cognitive load. The QA process should be systematic and documented. A comprehensive QA checklist is an essential tool, covering several key areas:

* **Content and Editorial Review:** Check for accuracy of all information, correct grammar and spelling, and consistency in terminology and style throughout the course.64  
* **Functional Testing:** Verify that all interactive elements—including buttons, links, quizzes, and navigation controls—work as expected. This testing should be conducted across all target web browsers (e.g., Chrome, Edge, Firefox) and devices (desktop, tablet, mobile) to ensure a consistent experience.65  
* **Multimedia Review:** Ensure all images and videos load correctly and are of high quality. Audio narration should be clear, at an appropriate volume, and correctly synchronized with on-screen animations and text.64  
* **Accessibility Audit:** Conduct a final, thorough check against the WCAG 2.2 AA compliance checklist. This includes verifying that all images have alt-text, videos have accurate captions, color contrast ratios are met, and the entire course is navigable using only a keyboard.65

#### **6.2. Pilot Testing: A Real-World Dress Rehearsal**

While QA testing focuses on finding technical and content errors, a pilot test focuses on evaluating the effectiveness of the learning experience itself.66 A pilot involves deploying the course to a small, representative group of learners from the target audience before the full-scale launch. This process provides invaluable feedback on aspects of the course that can only be assessed by actual users. A formal Pilot Test Plan should be created to guide this process, covering:

* **Pilot Goals:** Define the specific questions the pilot aims to answer. These should be focused on the learner experience, such as: "Is the pacing of the modules appropriate?" "Are the instructions for the final simulation clear and easy to follow?" or "Which activity did you find most and least engaging?".67  
* **Participant Selection:** Recruit a diverse group of 5-10 participants who accurately represent the target audience in terms of role, prior knowledge, and technical proficiency.67  
* **Data Collection Methods:** Use a mixed-methods approach to gather comprehensive feedback. Quantitative data can be collected from LMS analytics (e.g., completion times, quiz scores) and post-pilot surveys. Qualitative data, which often provides the richest insights, can be gathered through direct observation (watching a user navigate the course) or a structured debriefing session/focus group after the pilot is complete.67  
* **Debriefing:** The debrief session should be guided by open-ended questions designed to elicit candid feedback, such as: "What went well?" "What were some areas of opportunity or confusion?" "What is your single biggest takeaway?" and "What, if anything, was missing?".66

#### **6.3. The 90-Day Rollout Plan**

A strategic rollout builds excitement, communicates value, and drives adoption. Rather than simply making a course available in the LMS, a structured communication and engagement plan is needed. A 30-60-90 day framework provides a useful structure for this plan.

* **First 30 Days (Pre-Launch & Launch):** The focus is on building awareness and setting expectations. This phase includes communications to managers, explaining the purpose of the training and their role in supporting their teams. It also includes direct communication to the target learner audience, highlighting the "what's in it for me" (WIIFM), outlining the learning objectives, and providing clear instructions on how to access the course. The launch day itself should be a clear event.68  
* **Next 30 Days (Post-Launch Engagement):** The focus shifts to driving initial participation and building momentum. This involves sending scheduled reminder emails to those who have not yet started or completed the course. Sharing positive testimonials from pilot users or early adopters can create social proof and encourage others to engage. Providing managers with a toolkit—including talking points for team meetings and discussion questions—can help them reinforce the training's importance.70  
* **Final 30 Days (Embedding & Reinforcement):** The focus is on ensuring long-term adoption and application. This phase involves sharing early success stories or data that demonstrates the program's impact. Distributing job aids, checklists, or other performance support tools that reinforce the training content helps bridge the gap between learning and doing. This period is also the ideal time to launch the formal Level 3 (Behavior) and Level 4 (Results) evaluation processes.68

### **Chapter 7: Phase 5 \- Evaluation & Continuous Improvement**

The launch of an eLearning program is not the end of the development lifecycle; it is the beginning of a continuous cycle of measurement, analysis, and improvement. Evaluation is the process of systematically determining the merit, worth, and significance of the training. A robust evaluation strategy provides the data needed to demonstrate the value of L\&D to the organization and to make informed decisions about how to refine and enhance learning programs over time.

#### **7.1. Measuring Impact: The Kirkpatrick-Phillips Model**

The Kirkpatrick-Phillips Five-Level Evaluation Model is the industry-standard framework for conducting a comprehensive evaluation of training effectiveness.71 It provides a structured approach to measuring results at progressively deeper and more impactful levels. It is important to note that not every program requires evaluation at all five levels; the level of evaluation should be appropriate to the program's strategic importance and budget.71

The five levels are:

* **Level 1: Reaction:** This level measures how participants react to the training. The key question is, "Did they find it engaging, relevant, and favorable?" Data is typically collected through post-course satisfaction surveys, often called "smile sheets".29  
* **Level 2: Learning:** This level assesses the extent to which participants acquired the intended knowledge, skills, and attitudes. The key question is, "Did they learn it?" This is measured through assessments like pre- and post-tests, skill demonstrations, or project evaluations that are directly tied to the learning objectives.29  
* **Level 3: Behavior:** This level measures the degree to which participants apply what they learned back on the job. The key question is, "Are they using it?" Data collection for this level occurs weeks or months after the training and can include manager observations, 360-degree feedback, analysis of performance data (e.g., CRM records), and self-reported application.29  
* **Level 4: Results:** This level measures the impact of the training on business outcomes. The key question is, "Did it affect the bottom line?" This requires connecting the behavioral changes from Level 3 to the key performance indicators (KPIs) identified during the initial needs analysis, such as increases in sales, reductions in errors, or improvements in customer satisfaction.29 A key improvement of the Phillips model over the original Kirkpatrick model is the emphasis on isolating the effects of training from other factors (e.g., market changes, new technology) that could also influence these KPIs.74  
* **Level 5: Return on Investment (ROI):** This level, added by Jack Phillips, compares the monetary value of the Level 4 results to the total cost of the training program. The key question is, "Was the investment worthwhile?" This involves converting the business impact data into a monetary value and calculating the ROI using the formula: ROI(%)=Program CostsNet Program Benefits​×100.71

##### **Table 7.1: The Kirkpatrick-Phillips Evaluation Framework**

| Level | Key Question | Data Collection Methods | Example (Sales Training) | Strategic Importance |
| :---- | :---- | :---- | :---- | :---- |
| **1: Reaction** | Did they find the training relevant and engaging? | Post-course satisfaction survey (smile sheet); informal feedback. 29 | Survey questions rating the quality of the facilitator, relevance of content, and usability of the platform. | Provides immediate feedback for course improvement and gauges initial learner buy-in. |
| **2: Learning** | Did they acquire the intended knowledge and skills? | Pre/post knowledge tests; role-play simulations; final project rubric scores. 29 | A graded role-play simulation where the learner must successfully handle three different client objections. | Validates that learning objectives were met and that knowledge transfer occurred within the training environment. |
| **3: Behavior** | Are they applying the new skills and knowledge on the job? | Manager observation checklists; 360-degree feedback; analysis of CRM data. 29 | Manager confirms via a checklist that the salesperson is consistently using the new 4-step discovery call process 60 days post-training. | Confirms that learning is transferring to the workplace, which is a prerequisite for achieving business impact. |
| **4: Results** | Did their changed behavior impact key business metrics? | Analysis of business KPIs (e.g., sales data, customer satisfaction scores, employee retention). 29 | Analysis shows a 12% increase in the average deal size for trained salespeople compared to a control group. | Measures the tangible contribution of the training program to the organization's strategic goals. |
| **5: ROI** | Did the monetary value of the results exceed the program's cost? | Cost-benefit analysis; ROI calculation. 71 | The monetary value of the 12% increase in deal size is calculated and compared to the total cost of developing and delivering the training. | Provides a financial justification for the training investment, communicating the value of L\&D in a language business leaders understand. |

#### **7.2. Leveraging Learning Analytics for Continuous Improvement**

Beyond formal evaluation models, the data generated within the Learning Management System (LMS) offers a powerful, real-time source of insight for continuous program improvement. Learning analytics involves the measurement, collection, analysis, and reporting of data about learners and their contexts, for purposes of understanding and optimizing learning and the environments in which it occurs.75

Key metrics available in most modern LMS platforms can be categorized into two groups:

* **Engagement Metrics:** These metrics provide insight into how learners are interacting with the course content. They include course completion rates, time spent on specific activities or modules, drop-off points (e.g., where in a video learners stop watching), and which resources are most frequently accessed or downloaded.76  
* **Performance Metrics:** These metrics measure how well learners are mastering the content. They include scores on quizzes and assessments, pass/fail rates, the number of attempts required to pass an assessment, and analysis of which questions are most frequently answered incorrectly.80

This data enables a cycle of data-driven iteration. By analyzing these metrics, instructional designers can identify specific problems and opportunities for improvement without waiting for a formal program review.77 For example, a high drop-off rate on a particular video may indicate it is too long and should be re-edited into shorter chunks. A quiz question that is consistently answered incorrectly may be poorly worded or may signal that the corresponding content needs to be clarified. Analytics can also be used to identify at-risk learners—those who are falling behind or struggling with the material—allowing for proactive intervention and support.83 This continuous, data-informed refinement ensures that eLearning programs evolve and improve over time, becoming more effective and efficient with each iteration.

---

## **Part III: Governance & Program Management**

### **Chapter 8: The Operational Framework**

Developing a single high-quality eLearning course is a significant achievement; building and sustaining an entire ecosystem of effective learning programs requires a robust operational framework. This chapter details the project management methodologies, governance structures, and maintenance protocols necessary to manage an eLearning function efficiently, consistently, and at scale. A well-defined operational framework ensures that projects are delivered on time and on budget, that quality standards are maintained, and that the organization's investment in learning yields long-term value.

#### **8.1. Choosing Your Project Management Workflow: A Hybrid Approach**

Instructional design (ID) models provide a systematic framework for guiding the development process. The two most prominent models in the corporate L\&D space are ADDIE and SAM, which represent different philosophical approaches to project management.73

* **The ADDIE Model:** Standing for Analyze, Design, Develop, Implement, and Evaluate, ADDIE is a traditional, linear "waterfall" model that originated in the U.S. military.28 Each phase is completed sequentially, with formal sign-offs typically required before moving to the next. Its strengths lie in its structured, systematic nature, which is well-suited for large, complex projects where requirements are well-defined upfront and stakeholder alignment is critical.28 Its primary weakness is its rigidity; changes late in the process can be difficult and costly to implement.86  
* **The Successive Approximation Model (SAM):** Developed by Dr. Michael Allen as an alternative to ADDIE, SAM is an agile, iterative model.87 It emphasizes rapid prototyping and frequent review cycles. The process begins with a "Savvy Start" brainstorming session, quickly moves to designing and prototyping solutions, and then iterates through cycles of development, implementation, and evaluation.87 Its strength is its flexibility and speed, allowing for continuous feedback and adaptation. It is particularly well-suited for projects where requirements may be unclear at the outset or where innovation is a key goal.85

Rather than viewing these models as mutually exclusive, a pragmatic and highly effective approach is to adopt a **hybrid model**. This approach leverages the strengths of both frameworks. The overarching structure of the project can follow the five phases of **ADDIE** for governance, ensuring that a thorough strategic analysis is completed upfront and a comprehensive evaluation is planned for the end. However, within the **Design and Development** phases, the team can adopt the iterative, rapid prototyping cycles of **SAM**. This allows for the strategic rigor of ADDIE at the project level while enabling the flexibility and collaborative refinement of SAM at the content creation level.

##### **Table 8.1: Comparative Analysis of ID Models (ADDIE vs. SAM)**

| Characteristic | ADDIE (Waterfall) | SAM (Agile) | Hybrid Approach Recommendation |
| :---- | :---- | :---- | :---- |
| **Process Flow** | Linear and sequential; each phase must be completed before the next begins. 86 | Iterative and cyclical; involves repeated cycles of design, prototyping, and review. 87 | Use ADDIE's five phases for high-level project governance and milestones. Use SAM's iterative cycles within the Design and Development phases. |
| **Stakeholder Involvement** | Formal reviews and sign-offs at the end of each major phase. 28 | Continuous collaboration and feedback throughout the process. 88 | Use formal ADDIE-style sign-offs for strategic documents (e.g., Design Blueprint). Use frequent, informal SAM-style reviews for tactical assets (e.g., prototypes). |
| **Flexibility to Change** | Low; changes are difficult and costly to make once a phase is complete. 86 | High; designed to accommodate and incorporate changes based on frequent feedback. 88 | Accommodate strategic changes during the Analysis phase. Allow for tactical flexibility and refinement during the iterative Development sprints. |
| **Risk Management** | Front-loads risk mitigation in the Analysis and Design phases; requires high certainty upfront. 28 | Distributes risk across multiple, small iterations; better suited for projects with uncertainty. 73 | Mitigate strategic risks (e.g., misalignment with business goals) with a thorough ADDIE Analysis phase. Mitigate execution risks (e.g., ineffective interactions) with SAM's rapid prototyping and user testing. |
| **Ideal Project Type** | Large, complex projects with well-defined, stable requirements and formal stakeholder structures. | Projects where requirements are evolving, innovation is desired, or speed to market is critical. 87 | Most corporate eLearning projects, which require both strategic alignment and the flexibility to create engaging, user-tested learning experiences. |

#### **8.2. Version Control & Asset Management**

In a collaborative eLearning development environment involving IDs, SMEs, developers, and other stakeholders, managing project files can quickly become chaotic without a systematic approach. Version control is the practice of tracking and managing changes to instructional materials to prevent confusion from multiple file versions and to streamline collaboration.89 A robust version control strategy includes:

* **Standardized Naming Conventions:** Establish and enforce a clear, logical file naming convention for all project assets. A common format is CourseCode\_AssetName\_vX.X\_YYYY-MM-DD.ext (e.g., SALES101\_Module3-Quiz\_v1.2\_2025-08-22.docx).  
* **Centralized Repository:** All project files should be stored in a single, centralized location accessible to all team members. This could be a shared network drive, a cloud storage service, or a formal version control system (VCS) like Git, often used in conjunction with a platform like GitHub or Bitbucket.90 A VCS provides a complete change history, allows for branching and merging of workstreams, and enhances accountability.91  
* **Change Log Documentation:** Maintain a simple change log for each major deliverable (like the storyboard or design document). This log should briefly describe the changes made in each new version, who made them, and when, providing traceability for project decisions.

#### **8.3. Course Maintenance & Archival Policies**

eLearning courses are not static assets; they are dynamic resources that require ongoing attention to remain effective and accurate. A proactive governance plan for the entire lifecycle of a course, including its eventual retirement, is essential for maintaining quality and managing resources effectively.

* **Maintenance Plan:** Every eLearning program should have a defined maintenance plan from its inception.92 This plan should specify a schedule for periodic reviews (e.g., annually or biennially) to check for and update outdated information, such as policy changes, product updates, or changes in personnel.93 The plan should also include regular technical checks for broken links, compatibility issues with new browser versions, and proper functioning of all interactive elements.93 A Course Maintenance Runbook can be created to document the standard procedures for these reviews.  
* **Archival Policy:** Not all courses are meant to last forever. An archival policy should define the criteria for retiring a course, such as when the content becomes obsolete, the underlying technology is no longer supported, or learner demand drops below a certain threshold. The policy should also specify the data retention period for archived courses. Many organizations, for regulatory or accreditation purposes, are required to maintain learner records and course content for a set period, often 5 to 7 years, after a course is taken offline.94 The archival process involves removing the course from active view in the LMS while preserving all course content and student activity data in a secure, long-term storage location.94

---

## **Part IV: The Playbook Toolkit (Appendices)**

### **Appendix A: Training Needs Analysis (TNA) Template**

This template provides a structured format for gathering data to identify performance gaps and determine if a training solution is appropriate.

Project Title: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Date: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Analyst: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
**1\. Business Goal & Problem Statement**

* **What is the overarching business goal this analysis supports?** (e.g., Increase customer retention by 10%)  
* **What is the specific performance problem or opportunity being observed?** (e.g., Customer satisfaction scores related to technical support have declined by 15% in the last quarter.)

**2\. Target Audience Analysis**

* **Who is the target audience for this potential training?** (Job role, department, location)  
* **What is the approximate size of this audience?**  
* **What are their existing knowledge and skills related to this topic?** (Novice, Intermediate, Expert)  
* **What are their primary job responsibilities related to the problem?**  
* **What are the technical constraints or preferences of this audience?** (e.g., Access to desktop/mobile, preferred learning time)

**3\. Data Collection: Stakeholder Interviews (Manager/Leadership)**

* **Interviewee Name & Role:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
* **Key Questions:**  
  * From your perspective, what does successful performance look like in this area?  
  * What specific behaviors do top performers exhibit that others do not?  
  * What do you believe are the root causes of the current performance gap?  
  * What are the business consequences of this performance gap?  
  * What resources or tools are currently available to support performance? Are they being used?  
  * What would be the key metrics of success for a training intervention?

**4\. Data Collection: Performer Interviews / Focus Groups**

* **Interviewee(s) Name(s) & Role(s):** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
* **Key Questions:**  
  * Describe a typical day/week in your role related to \[the problem area\].  
  * What are the biggest challenges or obstacles you face when performing \[the task\]?  
  * What training have you received on this topic in the past? What was helpful/not helpful?  
  * What resources, tools, or support would help you perform this task more effectively?  
  * What do you think are the key skills needed to excel in this area?

**5\. Data Collection: Performance Data & Document Review**

* **Data/Documents Reviewed:** (e.g., CRM reports, quality assurance scores, existing process documentation, customer surveys)  
* **Key Findings:** (Summarize quantitative and qualitative findings from the data.)

**6\. Analysis & Summary**

* **Is this a knowledge/skill gap?** (Can they not do it?) or **Is this a motivational/environmental gap?** (Do they not want to do it, or are there systemic barriers?)  
* **Summary of Performance Gaps:** (List the specific knowledge, skills, or behaviors that are lacking.)  
  * Gap 1:  
  * Gap 2:  
  * Gap 3:  
* **Root Cause Analysis:** (Summarize the likely causes of the identified gaps.)

**7\. Recommendations**

* **Is training the appropriate solution?** (Yes/No/Partially)  
* **Recommended Solution(s):** (If yes, describe the proposed training intervention. If no, recommend non-training solutions like process improvement, job aids, or changes in management.)  
* **Proposed Business Outcome for Training:** (A measurable statement of the expected business impact.)  
* **Proposed High-Level Learning Objectives:** (Initial draft of what learners should be able to do.)

### **Appendix B: Assessment Blueprint Template**

This template ensures that your summative assessment is directly and comprehensively aligned with your course's learning objectives.

Course Title: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Assessment Title: Final Certification Exam

| Module / Unit | Learning Objective (LO) | Bloom's Level | Assessment Item(s) | Item Format | Points | Rationale / Notes |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **Module 1: Introduction to Project Scoping** | **LO 1.1:** Given a project brief, the learner will be able to **identify** the key stakeholders. | Remember | Q1, Q2 | Multiple Choice | 2 | Basic knowledge check. |
|  | **LO 1.2:** The learner will be able to **explain** the purpose of a project charter. | Understand | Q3 | Short Answer | 3 | Checks for comprehension of a core concept. |
|  | **LO 1.3:** Given a project scenario, the learner will be able to **apply** the SMART criteria to evaluate a project goal. | Apply | Q4 | Scenario Analysis | 5 | Assesses application of a key framework. |
| **Module 2: Risk Management** | **LO 2.1:** The learner will be able to **differentiate** between a risk and an issue. | Analyze | Q5 | Multiple Choice | 2 | Tests ability to distinguish between related concepts. |
|  | **LO 2.2:** Given a project plan, the learner will be able to **construct** a basic risk register identifying at least five potential risks. | Create | Performance Task 1 | Project Deliverable | 15 | Authentic assessment of a core skill. |
| **Module 3: Stakeholder Communication** | **LO 3.1:** The learner will be able to **evaluate** the effectiveness of a sample communication plan based on provided criteria. | Evaluate | Performance Task 2 | Case Study Critique | 10 | Higher-order thinking; assesses judgment. |
| **Total** |  |  |  |  | **37** |  |

**Blueprint Summary:**

* **Total Points:** 37  
* **Item Count:** 5 questions, 2 performance tasks  
* **Bloom's Level Distribution:**  
  * Remember: 1 (5%)  
  * Understand: 1 (8%)  
  * Apply: 1 (14%)  
  * Analyze: 1 (5%)  
  * Evaluate: 1 (27%)  
  * Create: 1 (41%)

### **Appendix C: Course Design Blueprint Template**

This document serves as the master plan for the eLearning program, securing stakeholder alignment before development begins.

**1\. Project Overview**

* **Course Title:**  
* **Project ID:**  
* **Version:**  
* **Last Updated:**  
* **Project Owner / Sponsor:**  
* **Instructional Designer(s):**  
* **Subject Matter Expert(s):**

**2\. Program Rationale & Business Impact**

* **Problem Statement:** (What performance gap is this program designed to address? Sourced from TNA.)  
* **Business Goal:** (What is the measurable business outcome this program will support?)  
* **Strategic Alignment:** (How does this program support broader departmental or organizational goals?)

**3\. Audience Analysis**

* **Primary Audience:** (Job role, department, experience level.)  
* **Prerequisite Knowledge/Skills:** (What must learners know or be able to do before starting this course?)  
* **Learner Motivation:** (What are the key "What's In It For Me?" factors for this audience?)  
* **Technical Environment:** (What devices, browsers, and bandwidth will learners be using?)

**4\. Learning Objectives**

* **Terminal Objective:** (The single, overarching performance goal for the entire program.)  
* **Enabling Objectives:** (The specific, measurable learning objectives, broken down by module, that support the terminal objective.)  
  * **Module 1:**  
    * LO 1.1:  
    * LO 1.2:  
  * **Module 2:**  
    * LO 2.1:  
    * LO 2.2:

**5\. Course Structure & Sequence (Course Map)**

* (Provide a high-level visual or outline of the course structure, showing the sequence of modules, key topics within each module, and any prerequisites or branching logic.)  
  * Welcome & Introduction Module  
  * Module 1:  
  * Module 2:  
  * ...  
  * Capstone Project / Final Assessment  
  * Conclusion & Resources Module

**6\. Assessment & Evaluation Strategy**

* **Formative Assessment Strategy:** (How will learning be checked and feedback provided throughout the course? e.g., knowledge checks, practice activities, peer reviews.)  
* **Summative Assessment Strategy:** (How will final mastery be measured? e.g., final exam, capstone project, simulation. Reference the Assessment Blueprint.)  
* **Evaluation Plan (Kirkpatrick-Phillips):**  
  * **Level 1 (Reaction):**  
  * **Level 2 (Learning):**  
  * **Level 3 (Behavior):**  
  * **Level 4 (Results):**  
  * **Level 5 (ROI):** (If applicable)

**7\. Instructional & Technical Specifications**

* **Instructional Approach:** (e.g., Scenario-based learning, guided discovery, microlearning.)  
* **Delivery Modality:** (e.g., Asynchronous eLearning, virtual instructor-led, blended.)  
* **Authoring Tool:** (e.g., Articulate Storyline, Adobe Captivate, Rise.)  
* **LMS Specifications:** (e.g., SCORM 1.2/2004, xAPI.)  
* **Accessibility Standard:** WCAG 2.2 Level AA.

**8\. High-Level Project Plan & Milestones**

| Phase | Key Deliverable | Estimated Completion Date |
| :---- | :---- | :---- |
| **Analysis** | TNA & Business Case Complete |  |
| **Design** | Course Design Blueprint Sign-off |  |
| **Development** | Storyboard Sign-off |  |
|  | Alpha Build Complete |  |
|  | Beta Build Complete |  |
| **Implementation** | QA & Pilot Testing Complete |  |
|  | Official Launch |  |
| **Evaluation** | Level 1 & 2 Data Collected |  |
|  | Level 3 Data Collection Begins |  |

**9\. Stakeholder Sign-Off**

* **Project Sponsor:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ Date: \_\_\_\_\_\_\_\_\_  
* **Lead SME:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ Date: \_\_\_\_\_\_\_\_\_  
* **L\&D Lead:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ Date: \_\_\_\_\_\_\_\_\_

### **Appendix D: eLearning Storyboard Template**

This template provides a screen-by-screen guide for the development team.

**Course:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ **Module:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ **Version:** \_\_\_\_\_\_\_\_\_

| Screen ID | On-Screen Text & Visuals | Audio/Narration Script | Developer Notes |
| :---- | :---- | :---- | :---- |
| **1.1** | **Title:** Module 1: Introduction to Project Scoping **Visual:** Graphic of a magnifying glass over a project document. Company logo in top right. | **(Music: Upbeat, professional intro music fades in and then fades to background.)** **VO:** Welcome to Module 1: Introduction to Project Scoping. In this module, you'll learn how to define the boundaries of a project to ensure its success. | \- **Navigation:** Next button is enabled. Menu is visible. \- **Timing:** Screen auto-advances after 5 seconds. \- **Accessibility:** Alt-text for graphic: "Magnifying glass over a project plan, symbolizing project scoping." |
| **1.2** | **Title:** Learning Objectives **On-Screen Text:** By the end of this module, you will be able to: \- Identify key stakeholders from a project brief. \- Explain the purpose of a project charter. \- Apply the SMART criteria to evaluate a project goal. | **VO:** By the end of this module, you will be able to identify key stakeholders, explain the purpose of a project charter, and apply the SMART criteria to evaluate a project goal. | \- **Navigation:** Next button enabled. \- **Interaction:** Each bullet point animates in as it is read by the narrator. |
| **1.3** | **Title:** Knowledge Check **On-Screen Text:** Which of the following is the BEST example of a "Specific" goal according to the SMART criteria? **Options:** A. Improve team morale. B. Increase website traffic by 15% in Q3. C. Become a market leader. D. Learn new software. | **(No VO)** | \- **Interaction Type:** Multiple Choice, single select. \- **Correct Answer:** B \- **Feedback (Correct):** "Correct\! This goal is specific, measurable, and time-bound." \- **Feedback (Incorrect):** "Not quite. A specific goal clearly states what needs to be accomplished. 'Improve team morale' is vague. Try again." |

### **Appendix E: Quality Assurance (QA) Checklist**

This checklist is used to conduct a final review of the eLearning course before pilot testing and launch.

**Project:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ **Reviewer:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_ **Date:** \_\_\_\_\_\_\_\_\_

| Category | Checklist Item | Status (Pass/Fail) | Comments |
| :---- | :---- | :---- | :---- |
| **Instructional Content** | Content is factually accurate and up-to-date. |  |  |
|  | All content from the storyboard is present. |  |  |
|  | Content is sequenced logically. |  |  |
|  | Instructions for all activities are clear and concise. |  |  |
| **Editorial** | No spelling or grammatical errors. |  |  |
|  | Punctuation and capitalization are consistent. |  |  |
|  | Terminology is used consistently throughout the course. |  |  |
| **Visuals & Branding** | All images, videos, and graphics are high quality and load correctly. |  |  |
|  | Visuals adhere to the company's branding guidelines (logos, colors, fonts). |  |  |
|  | Visual formatting (alignment, spacing) is consistent across all screens. |  |  |
| **Audio & Video** | Audio is clear, professionally recorded, and free of background noise. |  |  |
|  | Narration is correctly synchronized with on-screen animations and text. |  |  |
|  | Videos play correctly and include user controls (play, pause, volume). |  |  |
| **Functionality** | All navigation buttons (Next, Back, Menu, Exit) work correctly. |  |  |
|  | All hyperlinks lead to the correct, active web pages. |  |  |
|  | All interactive elements (quizzes, drag-and-drops, simulations) function as designed. |  |  |
|  | Scoring and feedback for assessments are accurate. |  |  |
| **Cross-Device Testing** | Course displays and functions correctly on Chrome. |  |  |
|  | Course displays and functions correctly on Edge / Firefox. |  |  |
|  | Course displays and functions correctly on a standard tablet (e.g., iPad). |  |  |
|  | Course displays and functions correctly on a standard mobile device (e.g., iPhone/Android). |  |  |
| **Accessibility (WCAG 2.2 AA)** | All images and meaningful graphics have descriptive alt-text. |  |  |
|  | All videos have accurate, synchronized captions. |  |  |
|  | All audio has a complete transcript available. |  |  |
|  | Color contrast meets the 4.5:1 minimum for normal text. |  |  |
|  | The entire course is navigable using only the keyboard. |  |  |
|  | A visible focus indicator is present for all interactive elements during keyboard navigation. |  |  |

### **Appendix F: WCAG 2.2 AA Compliance Checklist for eLearning**

A focused checklist for verifying key accessibility standards in an eLearning context.

**Course Title:** \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

| WCAG Principle | Success Criterion \# | Requirement | Verification Method | Status (Pass/Fail/NA) |
| :---- | :---- | :---- | :---- | :---- |
| **Perceivable** | 1.1.1 | All non-text content (images, charts) has a text alternative (alt-text). | Inspect element code; use a screen reader. |  |
|  | 1.2.2 | Captions are provided for all prerecorded video content. | Play video and verify captions are present, accurate, and synchronized. |  |
|  | 1.2.3 | Audio Description or Media Alternative is provided for prerecorded video. | Verify a descriptive audio track or a full text transcript is available. |  |
|  | 1.4.3 | Text and images of text have a contrast ratio of at least 4.5:1. | Use a color contrast checker tool on all text elements and backgrounds. |  |
|  | 1.4.4 | Text can be resized up to 200% without loss of content or functionality. | Use browser zoom function (Ctrl/Cmd \+) to 200% and check for text overlap or clipping. |  |
| **Operable** | 2.1.1 | All functionality is operable through a keyboard interface. | Unplug mouse. Navigate the entire course using only Tab, Shift+Tab, Enter, and Spacebar. |  |
|  | 2.4.7 | Any keyboard operable user interface has a visible focus indicator. | As you navigate with the keyboard, verify a clear visual outline appears around all interactive elements. |  |
|  | 2.5.8 | The size of the target for pointer inputs is at least 24 by 24 CSS pixels. | Use browser developer tools to measure the dimensions of all buttons and clickable links. |  |
| **Understandable** | 3.2.3 | Navigational mechanisms that are repeated on multiple pages occur in the same relative order. | Check that the position of the Menu, Next, and Back buttons is consistent on all screens. |  |
|  | 3.3.1 | If an input error is automatically detected, the item that is in error is identified and the error is described to the user in text. | Intentionally fail a quiz question and verify that clear, textual feedback is provided. |  |
| **Robust** | 4.1.2 | For all user interface components, the name and role can be programmatically determined. | Use a screen reader (e.g., NVDA, JAWS) to verify it correctly announces all buttons, links, and input fields. |  |

### **Appendix G: Pilot Test Plan & Feedback Form Template**

**Part 1: Pilot Test Plan**

* **Course Title:**  
* **Pilot Dates:**  
* **Pilot Facilitator:**  
* **Pilot Goals:**  
  1. Evaluate the clarity and effectiveness of the instructional content.  
  2. Assess the usability and engagement level of the interactive simulations.  
  3. Determine if the estimated time for completion (seat time) is accurate.  
  4. Identify any points of confusion or technical difficulties.  
* **Participants:** (List names and roles of 5-8 representative learners.)  
* **Schedule:**  
  * **Day 1:** Kickoff email sent to participants with instructions and access link.  
  * **Day 1-4:** Participants complete the eLearning course at their own pace.  
  * **Day 5:** Participants complete the feedback form.  
  * **Day 6:** 60-minute facilitated debrief session (virtual focus group).  
* **Data Collection Methods:**  
  * LMS Analytics (completion status, time on task, quiz scores).  
  * Pilot Test Feedback Form (quantitative and qualitative questions).  
  * Facilitated Debrief Session (recorded for analysis).

**Part 2: Pilot Test Feedback Form**

*Please complete this form after you have finished the course. Your honest feedback is crucial for improving this training.*

**Section 1: Overall Experience (Scale: 1=Strongly Disagree, 5=Strongly Agree)**

1. The course objectives were clear. (1 2 3 4 5\)  
2. The content was relevant to my job role. (1 2 3 4 5\)  
3. The course was engaging and held my attention. (1 2 3 4 5\)  
4. I feel more confident in my ability to \[key skill\] after taking this course. (1 2 3 4 5\)  
5. The estimated time to complete the course (X hours) was accurate. (1 2 3 4 5\)

Section 2: Content & Activities  
6\. What was the most valuable or useful part of the course? Why?  
7\. What was the least valuable or most confusing part of the course? Why?  
8\. How would you rate the interactive simulations? (Please provide specific feedback on what worked well and what could be improved).  
9\. Were there any topics you wish had been covered in more detail?  
Section 3: Technical & Usability  
10\. Did you experience any technical issues (e.g., videos not playing, broken links, slow loading times)? If so, please describe them.  
11\. Was the course easy to navigate? Were the instructions clear?  
Section 4: Final Thoughts  
12\. What is your single biggest recommendation for improving this course?  
13\. Is there any other feedback you would like to share?

### **Appendix H: 90-Day Rollout & Communication Plan Template**

| Phase | Timeline | Audience | Key Message | Communication Channel | Owner |
| :---- | :---- | :---- | :---- | :---- | :---- |
| **Phase 1: Pre-Launch & Launch** | **Days 1-30** |  |  |  |  |
|  | **Week 1** | Senior Leadership | Announce program launch, reiterate strategic alignment and expected business impact. | Executive Briefing / Email | L\&D Lead |
|  | **Week 2** | People Managers | Introduce the program, explain its benefits for their teams, and outline their role in supporting learners. Provide Manager Toolkit. | Manager-focused Webinar / Email | Program Manager |
|  | **Week 3** | All Learners | "Coming Soon" announcement. Build excitement, explain the WIIFM, and provide the launch date. | All-Company Newsletter / Intranet Post | Comms Team |
|  | **Week 4 (Launch)** | All Learners | "It's Here\!" announcement. Provide direct link to the course, instructions for enrollment, and support contacts. | Targeted Email Campaign | Program Manager |
| **Phase 2: Engagement** | **Days 31-60** |  |  |  |  |
|  | **Week 5** | Non-Starters | Gentle reminder to begin the course. Reiterate benefits and deadline. | Automated Email | LMS Admin |
|  | **Week 6** | People Managers | Share initial enrollment data for their teams. Encourage them to discuss the course in 1:1s. | Manager Dashboard / Email | Program Manager |
|  | **Week 7** | All Learners | Share a positive testimonial from an early completer. Highlight a key takeaway or job aid from the course. | Intranet Post / Team Chat | Comms Team |
|  | **Week 8** | In-Progress Learners | Encouragement to complete the course. Highlight upcoming modules. | Automated Email | LMS Admin |
| **Phase 3: Reinforcement** | **Days 61-90** |  |  |  |  |
|  | **Week 10** | All Learners | Share early success metrics or a story of on-the-job application. | All-Company Newsletter | L\&D Lead |
|  | **Week 12** | Course Completers | Congratulatory email. Link to advanced resources or community of practice. Solicit success stories. | Automated Email | LMS Admin |

### **Appendix I: Course Maintenance Runbook Template**

A runbook is a set of standardized procedures for routine tasks. This template outlines the process for the annual review of an eLearning course.

Runbook ID: C-MAINT-001  
Runbook Title: Annual eLearning Course Review  
Course: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Review Cycle: Annual, due by  
Owner: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_

| Step | Procedure | Verification | Assigned To | Status (Complete/Date) |
| :---- | :---- | :---- | :---- | :---- |
| **1.0: Initiate Review** |  |  |  |  |
| 1.1 | Create a copy of the course in the staging/testing environment of the LMS. | Staging course is created and accessible. | LMS Admin |  |
| 1.2 | Notify the designated SME and a sample user from the target audience that the review period has begun. Provide access links and a due date for feedback. | SMEs and user have been notified. | Program Manager |  |
| **2.0: Content Accuracy Review** |  |  |  |  |
| 2.1 | SME reviews all on-screen text, narration scripts, and downloadable resources for factual accuracy, policy updates, and relevance. | SME provides feedback via the designated review tool or document. | SME |  |
| 2.2 | SME verifies that any references to other systems, products, or personnel are still current. | SME confirms all references are up-to-date. | SME |  |
| **3.0: Technical & Functional Review** |  |  |  |  |
| 3.1 | Test all external hyperlinks to ensure they are not broken. | All links resolve to the correct page. | ID / Developer |  |
| 3.2 | Test all navigation and interactive elements (buttons, quizzes, etc.) in the latest versions of supported browsers (Chrome, Edge). | All interactions function as expected. | ID / Developer |  |
| 3.3 | Verify that all video and audio assets play correctly. | All media loads and plays without error. | ID / Developer |  |
| **4.0: Feedback Consolidation & Implementation** |  |  |  |  |
| 4.1 | Consolidate all feedback from the SME and sample user into a single action list. | Action list created and prioritized. | Program Manager |  |
| 4.2 | Implement all required changes in the source file (e.g., Storyline file). | Changes are made and documented in the version control log. | ID / Developer |  |
| 4.3 | Publish the updated course package. | New SCORM package is created. | ID / Developer |  |
| **5.0: Deployment** |  |  |  |  |
| 5.1 | Upload the new course package to the LMS, replacing the previous version. | New version is live in the LMS. | LMS Admin |  |
| 5.2 | Conduct a final smoke test of the live course to ensure successful deployment. | Live course functions correctly. | Program Manager |  |
| 5.3 | Communicate any significant updates to past and future learners, if necessary. | Communication sent. | Program Manager |  |
| 5.4 | Close the maintenance ticket and schedule the next annual review. | Ticket closed. Next review scheduled. | Program Manager |  |

### **Appendix J: Project Risk Register Template**

This template is a living document used to identify, analyze, and manage potential risks throughout the eLearning project lifecycle..97

Project: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Project Manager: \_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_\_  
Last Updated: \_\_\_\_\_\_\_\_\_

| Risk ID | Date Identified | Risk Description | Category (e.g., Schedule, Budget, Scope, Technology) | Likelihood (1-5) | Impact (1-5) | Risk Score (L x I) | Risk Response / Mitigation Plan | Owner | Status (Open, In Progress, Closed) |
| :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- | :---- |
| **001** | 2025-01-15 | SME is unresponsive or provides delayed feedback, causing delays in the storyboard and development phases. | Schedule, Scope | 4 | 4 | 16 | \- Establish clear review deadlines in the project kickoff. \- Schedule weekly 30-minute check-in meetings with SME. \- Identify a backup SME with the project sponsor. | Project Manager | Open |
| **002** | 2025-01-20 | The new interactive simulation requires a software plugin that is not standard on all employee computers. | Technology, Schedule | 2 | 5 | 10 | \- Work with IT to test and deploy the required plugin to the target audience before the pilot test. \- Develop a low-tech alternative activity if deployment fails. | IT Lead | In Progress |
| **003** | 2025-02-01 | Stakeholders request significant changes to learning objectives after the design blueprint has been signed off (scope creep). | Scope, Budget | 3 | 4 | 12 | \- Implement a formal change control process. \- Clearly communicate the impact of the change on the timeline and budget. \- Refer back to the signed-off design blueprint as the source of truth. | Project Manager | Open |

**Risk Matrix:**

* **Likelihood/Impact Scale:** 1=Very Low, 2=Low, 3=Medium, 4=High, 5=Very High  
* **Risk Score Thresholds:**  
  * **15-25 (High):** Requires immediate mitigation plan and frequent monitoring.  
  * **8-14 (Medium):** Requires a mitigation plan and regular monitoring.  
  * **1-7 (Low):** Accept risk and monitor periodically.

### **Appendix K: Annotated Bibliography of Key Sources**

This bibliography provides a summary and evaluation of the core research and frameworks that informed this playbook.

1. **Agarwal, P. K., Nunes, L. D., & Blunt, J. R. (2021). Retrieval practice consistently benefits student learning: A systematic review of applied research in schools and classrooms. *Educational Psychology Review*, 33(4), 1409–1453.**  
   * **Summary:** This systematic review synthesizes a large body of applied research on retrieval practice (the "testing effect"). It confirms that actively recalling information is a highly effective learning strategy across various age groups, subject domains, and educational settings, consistently outperforming passive restudying for long-term retention.  
   * **Evaluation & Relevance:** This is a foundational source for the playbook's emphasis on active, effortful learning strategies. Its focus on applied research in real-world classrooms provides strong evidence for recommending the integration of low-stakes quizzing and other retrieval-based activities into eLearning design for professional learners.  
2. **Bjork, R. A. (1994). Memory and metamemory considerations in the training of human beings. In J. Metcalfe & A. Shimamura (Eds.), *Metacognition: Knowing about knowing* (pp. 185–205). MIT Press.**  
   * **Summary:** This seminal chapter introduces the concept of "desirable difficulties." Bjork argues that learning conditions that introduce challenges and make initial performance more difficult (such as spacing, interleaving, and retrieval practice) can lead to more durable, long-term learning compared to easier, more fluent conditions.  
   * **Evaluation & Relevance:** This source provides the theoretical underpinning for Chapter 1.2. It is critical for understanding *why* strategies like spaced and retrieval practice work. Its distinction between short-term performance and long-term learning is essential for designing effective professional development and for communicating the rationale behind more challenging instructional designs to learners and stakeholders.  
3. **Mayer, R. E. (2014). *The Cambridge handbook of multimedia learning* (2nd ed.). Cambridge University Press.**  
   * **Summary:** This handbook, and Mayer's broader body of work, is the definitive resource on how to design instructional materials that combine words and pictures for optimal learning. It details the cognitive theory of multimedia learning and outlines specific, evidence-based principles for design (e.g., Coherence, Modality, Personalization).  
   * **Evaluation & Relevance:** This work is the primary source for the practical design guidance in Chapter 5.1. It translates cognitive science into actionable rules for instructional designers, providing a clear, research-backed rationale for making specific choices about layout, narration, and graphics to manage cognitive load and facilitate learning.  
4. **Ryan, R. M., & Deci, E. L. (2017). *Self-determination theory: Basic psychological needs in motivation, development, and wellness*. Guilford Press.**  
   * **Summary:** This book provides a comprehensive overview of Self-Determination Theory (SDT), a major theory of human motivation. It details the three basic psychological needs—autonomy, competence, and relatedness—and explains how their satisfaction or frustration impacts motivation, engagement, and well-being across various life domains, including education.  
   * **Evaluation & Relevance:** SDT is the foundational framework for the playbook's approach to learner motivation and engagement (Chapter 1.3). It moves beyond simplistic gamification or rewards-based models to provide a more nuanced and powerful strategy for designing learning environments that foster intrinsic and self-determined motivation, which is particularly important for adult learners.  
5. **Sweller, J., van Merriënboer, J. J. G., & Paas, F. (2019). Cognitive architecture and instructional design: 20 years later. *Educational Psychology Review*, 31(2), 261–292.**  
   * **Summary:** This article provides a contemporary review and update on Cognitive Load Theory from its originators. It summarizes two decades of research, refines the core concepts (including the different types of cognitive load), and discusses the implications of the theory for modern instructional design, including the expertise reversal effect.  
   * **Evaluation & Relevance:** This is a primary source for the playbook's discussion of Cognitive Load Theory (Chapter 1.1). Its authority and comprehensiveness ensure that the playbook's recommendations are based on the most current understanding of this critical learning science principle. It provides the scientific justification for many design decisions, such as minimalism, chunking, and the use of worked examples.  
6. **Wiggins, G., & McTighe, J. (2005). *Understanding by design* (2nd ed.). ASCD.**  
   * **Summary:** This book details the "Backward Design" framework for curriculum and instructional design. It advocates for a three-stage process that begins with identifying desired results (learning goals), then determining acceptable evidence of learning (assessment), and only then planning the learning experiences and instruction.  
   * **Evaluation & Relevance:** Backward Design provides the core strategic framework for the entire program lifecycle detailed in Part II of the playbook. Its "begin with the end in mind" philosophy is a powerful antidote to common design flaws, such as creating content-driven courses that are disconnected from meaningful outcomes. It provides a logical and defensible process for ensuring that all elements of an eLearning program are purposeful and aligned.

#### **Works cited**

1. Cognitive load theory as an aid for instructional design, accessed August 20, 2025, [https://ajet.org.au/index.php/AJET/article/download/2322/1146/7314](https://ajet.org.au/index.php/AJET/article/download/2322/1146/7314)  
2. Cognitive Load Theory: How to Optimize Learning \- Let's Go Learn, accessed August 20, 2025, [https://www.letsgolearn.com/education-reform/cognitive-load-theory-how-to-optimize-learning/](https://www.letsgolearn.com/education-reform/cognitive-load-theory-how-to-optimize-learning/)  
3. 12 Principles of Multimedia Learning \- University of Hartford, accessed August 20, 2025, [https://www.hartford.edu/faculty-staff/faculty/fcld/\_files/12%20Principles%20of%20Multimedia%20Learning.pdf](https://www.hartford.edu/faculty-staff/faculty/fcld/_files/12%20Principles%20of%20Multimedia%20Learning.pdf)  
4. Dual Coding: Exploring opportunities to deliver learning content in ..., accessed August 20, 2025, [https://www.ntu.ac.uk/about-us/teaching/academic-development-and-quality/cadq-blogs/dual-coding-exploring-opportunities-to-deliver-learning-content-in-now](https://www.ntu.ac.uk/about-us/teaching/academic-development-and-quality/cadq-blogs/dual-coding-exploring-opportunities-to-deliver-learning-content-in-now)  
5. Learning Through Chunking: Mastering Content Breakdown ..., accessed August 20, 2025, [https://elearningindustry.com/mastering-content-breakdown-enhancing-learning-through-chunking](https://elearningindustry.com/mastering-content-breakdown-enhancing-learning-through-chunking)  
6. Strategies for Effective Instructional Sequencing: Structuring Content ..., accessed August 20, 2025, [https://yourelearningworld.com/strategies-for-effective-instructional-sequencing-structuring-content-for-optimal-learning/](https://yourelearningworld.com/strategies-for-effective-instructional-sequencing-structuring-content-for-optimal-learning/)  
7. Worked Examples | Teaching \+ Learning Lab, accessed August 20, 2025, [https://tll.mit.edu/teaching-resources/how-people-learn/worked-examples/](https://tll.mit.edu/teaching-resources/how-people-learn/worked-examples/)  
8. AERO Tried and Tested – Spacing and retrieval practice, accessed August 20, 2025, [https://www.edresearch.edu.au/sites/default/files/2021-09/tried-tested-spacing-retrieval-practice-aero.pdf](https://www.edresearch.edu.au/sites/default/files/2021-09/tried-tested-spacing-retrieval-practice-aero.pdf)  
9. Using Retrieval Practice to Increase Student Learning, accessed August 20, 2025, [https://ctl.wustl.edu/resources/using-retrieval-practice-to-increase-student-learning/](https://ctl.wustl.edu/resources/using-retrieval-practice-to-increase-student-learning/)  
10. Desirable difficulty \- Wikipedia, accessed August 20, 2025, [https://en.wikipedia.org/wiki/Desirable\_difficulty](https://en.wikipedia.org/wiki/Desirable_difficulty)  
11. Retrieval Practice in Classroom Settings: A Review of Applied Research \- Frontiers, accessed August 20, 2025, [https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2019.00005/full](https://www.frontiersin.org/journals/education/articles/10.3389/feduc.2019.00005/full)  
12. Can retrieval practice improve student performance within an A-level psychology classroom? \- My College, accessed August 20, 2025, [https://my.chartered.college/impact\_article/retrieval-practice-vs-restudy-an-authentic-classroom-investigation/](https://my.chartered.college/impact_article/retrieval-practice-vs-restudy-an-authentic-classroom-investigation/)  
13. The Science of Learning Learning Theory: Interleaving Definition: Switching between ideas or concepts while studying to avoid, accessed August 20, 2025, [https://www.med.uvm.edu/docs/interleavingb88a77f025ec4640b3e92d87e25fe5d9/active\_learning/the\_science\_of\_learning-interleaving.pdf?sfvrsn=b0663169\_0](https://www.med.uvm.edu/docs/interleavingb88a77f025ec4640b3e92d87e25fe5d9/active_learning/the_science_of_learning-interleaving.pdf?sfvrsn=b0663169_0)  
14. Desirable Difficulty \- Davidson-Davie Community College, accessed August 20, 2025, [https://www.davidsondavie.edu/desirable-difficulty/](https://www.davidsondavie.edu/desirable-difficulty/)  
15. Retrieval Practice: A Tool for Teaching the Control-of-Variables Strategy in Science Classrooms? \- Taylor & Francis Online, accessed August 20, 2025, [https://www.tandfonline.com/doi/full/10.1080/00220973.2024.2392684](https://www.tandfonline.com/doi/full/10.1080/00220973.2024.2392684)  
16. Full article: Retrieval-based learning versus discussion; which review practice will better enhance primary school students' knowledge of scientific content?, accessed August 20, 2025, [https://www.tandfonline.com/doi/full/10.1080/09500693.2023.2283906](https://www.tandfonline.com/doi/full/10.1080/09500693.2023.2283906)  
17. Pathways to Student Motivation: A Meta-Analysis of Antecedents of ..., accessed August 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC8935530/](https://pmc.ncbi.nlm.nih.gov/articles/PMC8935530/)  
18. Universal Design for Learning (UDL) Guidelines | Texas Education ..., accessed August 20, 2025, [https://tea.texas.gov/academics/special-student-populations/special-education/universal-design-for-learning-udl-guidelines](https://tea.texas.gov/academics/special-student-populations/special-education/universal-design-for-learning-udl-guidelines)  
19. Mastery learning \- Wikipedia, accessed August 20, 2025, [https://en.wikipedia.org/wiki/Mastery\_learning](https://en.wikipedia.org/wiki/Mastery_learning)  
20. Formative Assessment and Feedback | Teaching Commons, accessed August 20, 2025, [https://teachingcommons.stanford.edu/teaching-guides/foundations-course-design/feedback-and-assessment/formative-assessment-and-feedback](https://teachingcommons.stanford.edu/teaching-guides/foundations-course-design/feedback-and-assessment/formative-assessment-and-feedback)  
21. ED509138 \- How to Give Effective Feedback to Your Students, Association for Supervision and Curriculum Development, 2008-Sep \- ERIC, accessed August 20, 2025, [https://eric.ed.gov/?id=ED509138](https://eric.ed.gov/?id=ED509138)  
22. How to Give Feedback | Teaching \+ Learning Lab, accessed August 20, 2025, [https://tll.mit.edu/teaching-resources/assess-learning/how-to-give-feedback/](https://tll.mit.edu/teaching-resources/assess-learning/how-to-give-feedback/)  
23. Universal Design for Learning (UDL) | Center for the Advancement ..., accessed August 20, 2025, [https://teaching.uic.edu/cate-teaching-guides/inclusive-equity-minded-teaching-practices/universal-design-for-learning-udl/](https://teaching.uic.edu/cate-teaching-guides/inclusive-equity-minded-teaching-practices/universal-design-for-learning-udl/)  
24. Page 3: UDL Principles \- IRIS Center, accessed August 20, 2025, [https://iris.peabody.vanderbilt.edu/module/udl/cresource/q1/p03/](https://iris.peabody.vanderbilt.edu/module/udl/cresource/q1/p03/)  
25. Web Content Accessibility Guidelines (WCAG) 2.2 \- W3C, accessed August 20, 2025, [https://www.w3.org/TR/WCAG22/](https://www.w3.org/TR/WCAG22/)  
26. Backward Design | WMUx | Western Michigan University, accessed August 20, 2025, [https://wmich.edu/x/teaching-learning/teaching-resources/backward-design](https://wmich.edu/x/teaching-learning/teaching-resources/backward-design)  
27. Backward Design | Center for Teaching and Learning | Kent State University, accessed August 20, 2025, [https://www.kent.edu/ctl/backward-design](https://www.kent.edu/ctl/backward-design)  
28. ADDIE Model Explained: All You Need to Know \[+ FREE Template\] \- AIHR, accessed August 20, 2025, [https://www.aihr.com/blog/addie-model/](https://www.aihr.com/blog/addie-model/)  
29. What is the Kirkpatrick Model? Learn the 4 Levels of Evaluation, accessed August 20, 2025, [https://www.ardentlearning.com/blog/what-is-the-kirkpatrick-model](https://www.ardentlearning.com/blog/what-is-the-kirkpatrick-model)  
30. www.iaea.org, accessed August 20, 2025, [https://www.iaea.org/sites/default/files/18/03/competency-framework.pdf](https://www.iaea.org/sites/default/files/18/03/competency-framework.pdf)  
31. Competency Frameworks | Thomas.co, accessed August 20, 2025, [https://www.thomas.co/resources/type/hr-guides/competency-frameworks](https://www.thomas.co/resources/type/hr-guides/competency-frameworks)  
32. 9-Step Guide to an Effective Competency Mapping Process \- Deel, accessed August 20, 2025, [https://www.deel.com/blog/competency-mapping-process/](https://www.deel.com/blog/competency-mapping-process/)  
33. Build a competency map in 6 steps \- Adam Fard UX Studio, accessed August 20, 2025, [https://adamfard.com/blog/competency-map](https://adamfard.com/blog/competency-map)  
34. Active Verbs for Bloom's Revised Taxonomy \- Centre for Teaching Support & Innovation, accessed August 20, 2025, [https://teaching.utoronto.ca/resources/active-verbs-for-blooms-revised-taxonomy/](https://teaching.utoronto.ca/resources/active-verbs-for-blooms-revised-taxonomy/)  
35. Bloom's Revised Taxonomy Action Verbs, accessed August 20, 2025, [https://www.jsu.edu/online/faculty/blooms-revised-taxonomy-verb-list.pdf](https://www.jsu.edu/online/faculty/blooms-revised-taxonomy-verb-list.pdf)  
36. Bloom's Taxonomy of Measurable Verbs, accessed August 20, 2025, [https://www.utica.edu/academic/Assessment/new/Blooms%20Taxonomy%20-%20Best.pdf](https://www.utica.edu/academic/Assessment/new/Blooms%20Taxonomy%20-%20Best.pdf)  
37. Distinguishing Between Formative and Summative Assessments, accessed August 20, 2025, [https://citl.illinois.edu/citl-101/teaching-learning/resources/teaching-across-modalities/teaching-tips-articles/teaching-tips/2021/08/11/distinguishing-between-formative-and-summative-assessments](https://citl.illinois.edu/citl-101/teaching-learning/resources/teaching-across-modalities/teaching-tips-articles/teaching-tips/2021/08/11/distinguishing-between-formative-and-summative-assessments)  
38. Formative & Summative Assessments | Poorvu Center for Teaching ..., accessed August 20, 2025, [https://poorvucenter.yale.edu/teaching/teaching-resource-library/formative-summative-assessments](https://poorvucenter.yale.edu/teaching/teaching-resource-library/formative-summative-assessments)  
39. Authentic Assessments | Center for the Advancement of Teaching ..., accessed August 20, 2025, [https://teaching.uic.edu/cate-teaching-guides/assessment-grading-practices/authentic-assessments/](https://teaching.uic.edu/cate-teaching-guides/assessment-grading-practices/authentic-assessments/)  
40. Designing Authentic Assessment | Ohio University, accessed August 20, 2025, [https://www.ohio.edu/center-teaching-learning/assessment/approaches-methods/designing-authentic-assessment](https://www.ohio.edu/center-teaching-learning/assessment/approaches-methods/designing-authentic-assessment)  
41. Enterprise Portal: Create an Assessment Blueprint – ExamSoft, accessed August 20, 2025, [https://support.examsoft.com/hc/en-us/articles/11168066027661-Enterprise-Portal-Create-an-Assessment-Blueprint](https://support.examsoft.com/hc/en-us/articles/11168066027661-Enterprise-Portal-Create-an-Assessment-Blueprint)  
42. Test Blueprint Guide \- Clemson University, accessed August 20, 2025, [https://media.clemson.edu/otei/documents/teaching\_review\_resources/test\_blueprint\_guide\_final.pdf](https://media.clemson.edu/otei/documents/teaching_review_resources/test_blueprint_guide_final.pdf)  
43. Teaching a Course as a Narrative Arc | Faculty Focus, accessed August 20, 2025, [https://www.facultyfocus.com/articles/course-design-ideas/teaching-a-course-as-a-narrative-arc/](https://www.facultyfocus.com/articles/course-design-ideas/teaching-a-course-as-a-narrative-arc/)  
44. pmc.ncbi.nlm.nih.gov, accessed August 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC4598645/\#:\~:text=Gagne%20suggested%209%20events%20of,and%20enhance%20retention%20and%20transfer.](https://pmc.ncbi.nlm.nih.gov/articles/PMC4598645/#:~:text=Gagne%20suggested%209%20events%20of,and%20enhance%20retention%20and%20transfer.)  
45. Using Gagne's 9 Events of Instruction to Enhance Student ..., accessed August 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC4598645/](https://pmc.ncbi.nlm.nih.gov/articles/PMC4598645/)  
46. Merrill's Principles of Instruction \- Education Library, accessed August 20, 2025, [https://educationlibrary.org/merrills-principles-of-instruction/](https://educationlibrary.org/merrills-principles-of-instruction/)  
47. Merrill's Principles Of Instruction: The Definitive Guide \- eLearning Industry, accessed August 20, 2025, [https://elearningindustry.com/merrills-principles-instruction-definitive-guide](https://elearningindustry.com/merrills-principles-instruction-definitive-guide)  
48. Repeating and growing patterns: Foundation: Planning tool \- Mathematics Hub, accessed August 20, 2025, [https://www.mathematicshub.edu.au/planning-tool/f/algebra/repeating-and-growing-patterns/](https://www.mathematicshub.edu.au/planning-tool/f/algebra/repeating-and-growing-patterns/)  
49. Module Blueprints \- Online Resource Portal \- UGA, accessed August 20, 2025, [https://faculty.online.uga.edu/faculty/technical-resources/module-blueprints/](https://faculty.online.uga.edu/faculty/technical-resources/module-blueprints/)  
50. Course Design Blueprint, accessed August 20, 2025, [https://www.sfasu.edu/docs/ctl/course-design-blueprint.pdf](https://www.sfasu.edu/docs/ctl/course-design-blueprint.pdf)  
51. Microlearning: Top Examples and Best Practices for 2025 | Blog Gamfi, accessed August 20, 2025, [https://gamfi.com/blog/microlearning-top-examples-and-best-practices-for-2025](https://gamfi.com/blog/microlearning-top-examples-and-best-practices-for-2025)  
52. Instructional Design Templates — Latest documentation, accessed August 20, 2025, [https://docs.openedx.org/en/latest/educators/concepts/instructional\_design/id\_templates.html](https://docs.openedx.org/en/latest/educators/concepts/instructional_design/id_templates.html)  
53. How to Create an Instructional Design Document | Examples \+ ..., accessed August 20, 2025, [https://community.elearningacademy.io/c/knowledge-base/how-to-create-an-instructional-design-document](https://community.elearningacademy.io/c/knowledge-base/how-to-create-an-instructional-design-document)  
54. Online Course Planning Blueprint, accessed August 20, 2025, [https://onlineteaching.umich.edu/articles/online-course-planning-blueprint/](https://onlineteaching.umich.edu/articles/online-course-planning-blueprint/)  
55. Sample Course Design Plan \- National Highway Institute, accessed August 20, 2025, [https://www.nhi.fhwa.dot.gov/resources/docs/Course\_Design\_Plan.docx](https://www.nhi.fhwa.dot.gov/resources/docs/Course_Design_Plan.docx)  
56. 11 Best Practices for E-Learning Storyboarding | Articulate ..., accessed August 20, 2025, [https://community.articulate.com/blog/articles/11-best-practices-for-e-learning-storyboarding/1108555](https://community.articulate.com/blog/articles/11-best-practices-for-e-learning-storyboarding/1108555)  
57. Storyboarding for Instructional Designers: Proven Ways to Maximize Impact | Maestro, accessed August 20, 2025, [https://maestrolearning.com/blogs/storyboarding-for-instructional-design/](https://maestrolearning.com/blogs/storyboarding-for-instructional-design/)  
58. Mayer's 12 Multimedia Principles Decoded \- Elai.io, accessed August 20, 2025, [https://elai.io/mayers-12-multimedia-principles-decoded/](https://elai.io/mayers-12-multimedia-principles-decoded/)  
59. Branching Out with Interactive Scenarios | Online Course ..., accessed August 20, 2025, [https://www.vanderbilt.edu/cdr/module1/branching-out-with-interactive-scenarios/](https://www.vanderbilt.edu/cdr/module1/branching-out-with-interactive-scenarios/)  
60. Branching Scenarios: What You Need To Know \- eLearning Industry, accessed August 20, 2025, [https://elearningindustry.com/branching-scenarios-need-know](https://elearningindustry.com/branching-scenarios-need-know)  
61. Deliberate Practice as a Theoretical Framework for Interprofessional Experiential Education, accessed August 20, 2025, [https://pmc.ncbi.nlm.nih.gov/articles/PMC4935723/](https://pmc.ncbi.nlm.nih.gov/articles/PMC4935723/)  
62. What is Deliberate Practice? — Sentio University, accessed August 20, 2025, [https://sentio.org/what-is-deliberate-practice](https://sentio.org/what-is-deliberate-practice)  
63. Collaboration is Key to Create Great Learning Experiences, accessed August 20, 2025, [https://www.shiftelearning.com/blog/collaboration-is-key-to-create-great-learning-experiences](https://www.shiftelearning.com/blog/collaboration-is-key-to-create-great-learning-experiences)  
64. eLearning Quality Assurance Checklist with Use Cases \- zipBoard, accessed August 20, 2025, [https://zipboard.co/blog/elearning/elearning-qa-checklist/](https://zipboard.co/blog/elearning/elearning-qa-checklist/)  
65. Elearning QA Checklist \- Endurance Learning, accessed August 20, 2025, [https://endurancelearning.com/blog/elearning-qa-checklist/](https://endurancelearning.com/blog/elearning-qa-checklist/)  
66. The Value of Pilot Testing \- Endurance Learning, accessed August 20, 2025, [https://endurancelearning.com/blog/the-value-of-a-pilot/](https://endurancelearning.com/blog/the-value-of-a-pilot/)  
67. How to run an effective pilot program for a learning platform \- Tribal Habits, accessed August 20, 2025, [https://tribalhabits.com/pilot-program-for-a-learning-platform/](https://tribalhabits.com/pilot-program-for-a-learning-platform/)  
68. The Complete Guide to Creating Effective 30-60-90 Day Plans for New Employees | TMI, accessed August 20, 2025, [https://www.tmi.org/blogs/the-complete-guide-to-creating-effective-30-60-90-day-plans-for-new-employees](https://www.tmi.org/blogs/the-complete-guide-to-creating-effective-30-60-90-day-plans-for-new-employees)  
69. 30-60-90 Day Plan Template & Guide \[+ Free PowerPoint & Excel Download\] \- AIHR, accessed August 20, 2025, [https://www.aihr.com/blog/30-60-90-day-plan-template/](https://www.aihr.com/blog/30-60-90-day-plan-template/)  
70. The 30-60-90 Day Plan Explained (+Free Templates) \- Tracking Time, accessed August 20, 2025, [https://trackingtime.co/best-practices/30-60-90-day-plan-templates.html](https://trackingtime.co/best-practices/30-60-90-day-plan-templates.html)  
71. Phillips ROI Model: The 5 Levels of Training Evaluation (2025), accessed August 20, 2025, [https://whatfix.com/blog/phillips-roi-model/](https://whatfix.com/blog/phillips-roi-model/)  
72. Evaluation, Resourcing & ROI \- Air Force Learning Professionals, accessed August 20, 2025, [https://www.learningprofessionals.af.mil/Resources/Evaluation-Resourcing-ROI/](https://www.learningprofessionals.af.mil/Resources/Evaluation-Resourcing-ROI/)  
73. Instructional Design Models, accessed August 20, 2025, [https://www.instructionaldesigncentral.com/instructionaldesignmodels](https://www.instructionaldesigncentral.com/instructionaldesignmodels)  
74. The Phillips ROI Model: An Analysis \- MATC Group Inc, accessed August 20, 2025, [https://www.matcgroup.com/instructional-design/the-phillips-roi-model-an-analysis/](https://www.matcgroup.com/instructional-design/the-phillips-roi-model-an-analysis/)  
75. Using Learning Analytics to Assess Student Learning in Online Courses \- ERIC, accessed August 20, 2025, [https://files.eric.ed.gov/fulltext/EJ1110545.pdf](https://files.eric.ed.gov/fulltext/EJ1110545.pdf)  
76. 11 LMS Reports You Need to Track and Optimize in 2025, accessed August 20, 2025, [https://www.educate-me.co/blog/lms-reporting](https://www.educate-me.co/blog/lms-reporting)  
77. Learning Analytics: The Ultimate Guide | DLI Blog, accessed August 20, 2025, [https://www.digitallearninginstitute.com/blog/learning-analytics-the-ultimate-guide](https://www.digitallearninginstitute.com/blog/learning-analytics-the-ultimate-guide)  
78. Leveraging Video Analytics to Improve E-Learning Effectiveness \- The Blog \- Cincopa, accessed August 20, 2025, [https://www.cincopa.com/blog/leveraging-video-analytics-to-improve-e-learning-effectiveness/](https://www.cincopa.com/blog/leveraging-video-analytics-to-improve-e-learning-effectiveness/)  
79. Measuring E-Learning Effectiveness: What The Data Really Tells You \- Analytify, accessed August 20, 2025, [https://analytify.io/measuring-e-learning-effectiveness/](https://analytify.io/measuring-e-learning-effectiveness/)  
80. How to use LMS data to support curriculum development \- Watermark Insights, accessed August 20, 2025, [https://www.watermarkinsights.com/resources/blog/how-to-use-lms-data-to-support-curriculum-development/](https://www.watermarkinsights.com/resources/blog/how-to-use-lms-data-to-support-curriculum-development/)  
81. Top eLearning Data Collection Metrics To Track For Better Outcomes, accessed August 20, 2025, [https://elearningindustry.com/top-elearning-data-collection-metrics-to-track-for-better-outcomes](https://elearningindustry.com/top-elearning-data-collection-metrics-to-track-for-better-outcomes)  
82. 5 Reasons Why Learning Analytics Are Important For eLearning, accessed August 20, 2025, [https://elearningindustry.com/5-reasons-why-learning-analytics-are-important-for-elearning](https://elearningindustry.com/5-reasons-why-learning-analytics-are-important-for-elearning)  
83. Identifying At-Risk Students Using Data Analytics \- SchoolAnalytix, accessed August 20, 2025, [https://www.schoolanalytix.com/identifying-at-risk-students-using-data-analytics-2/](https://www.schoolanalytix.com/identifying-at-risk-students-using-data-analytics-2/)  
84. The effectiveness of learning analytics for identifying at-risk students in higher education, accessed August 20, 2025, [https://www.researchgate.net/publication/337061501\_The\_effectiveness\_of\_learning\_analytics\_for\_identifying\_at-risk\_students\_in\_higher\_education](https://www.researchgate.net/publication/337061501_The_effectiveness_of_learning_analytics_for_identifying_at-risk_students_in_higher_education)  
85. Understanding Theories and Models of Instructional Design \- Keiser University, accessed August 20, 2025, [https://www.keiseruniversity.edu/theories-models-instructional-design/](https://www.keiseruniversity.edu/theories-models-instructional-design/)  
86. ADDIE Model: 5 Stages of Instructional Design (2025) \- Whatfix, accessed August 20, 2025, [https://whatfix.com/blog/addie-model-instructional-design/](https://whatfix.com/blog/addie-model-instructional-design/)  
87. The complete guide to the Successive Approximation Model (SAM ..., accessed August 20, 2025, [https://www.howtoo.co/posts/the-complete-guide-to-the-successive-approximation-model-sam-of-instructional-design](https://www.howtoo.co/posts/the-complete-guide-to-the-successive-approximation-model-sam-of-instructional-design)  
88. The SAM (Successive Approximation Model) Approach to eLearning \- ELM Learning, accessed August 20, 2025, [https://elmlearning.com/hub/instructional-design/sam-successive-approximation-model/](https://elmlearning.com/hub/instructional-design/sam-successive-approximation-model/)  
89. www.coursensu.com, accessed August 20, 2025, [https://www.coursensu.com/instructional-design-terms/version-control\#:\~:text=Version%20control%20allows%20educators%20to,streamlines%20collaboration%20among%20team%20members.](https://www.coursensu.com/instructional-design-terms/version-control#:~:text=Version%20control%20allows%20educators%20to,streamlines%20collaboration%20among%20team%20members.)  
90. Version Control For Teaching \- Meegle, accessed August 20, 2025, [https://www.meegle.com/en\_us/topics/version-control/version-control-for-teaching](https://www.meegle.com/en_us/topics/version-control/version-control-for-teaching)  
91. What is version control | Atlassian Git Tutorial, accessed August 20, 2025, [https://www.atlassian.com/git/tutorials/what-is-version-control](https://www.atlassian.com/git/tutorials/what-is-version-control)  
92. eLearning Maintenance Questionnaire \- TrainingFolks, accessed August 20, 2025, [https://www.trainingfolks.com/-elearning-maintenance-questionnaire](https://www.trainingfolks.com/-elearning-maintenance-questionnaire)  
93. E-Learning Maintenance Checklist | Articulate \- Community, accessed August 20, 2025, [https://community.articulate.com/blog/articles/e-learning-maintenance-checklist/1098386](https://community.articulate.com/blog/articles/e-learning-maintenance-checklist/1098386)  
94. Drexel Learn Course Removal and Archive Procedure | Information Technology, accessed August 20, 2025, [https://drexel.edu/it/about/policies/procedures/course-archive/](https://drexel.edu/it/about/policies/procedures/course-archive/)  
95. Canvas Course Archival and Retention \- University Registrar, accessed August 20, 2025, [https://registrar.jhu.edu/policies/canvas-course-archival-and-retention/](https://registrar.jhu.edu/policies/canvas-course-archival-and-retention/)  
96. Archiving course enrollments \- Docebo Help, accessed August 20, 2025, [https://help.docebo.com/hc/en-us/articles/11521889929490-Archiving-course-enrollments](https://help.docebo.com/hc/en-us/articles/11521889929490-Archiving-course-enrollments)  
97. Risk Register In Project Management \- Simpliaxis, accessed August 20, 2025, [https://www.simpliaxis.com/resources/risk-register-in-project-management](https://www.simpliaxis.com/resources/risk-register-in-project-management)  
98. Risk Register: A Project Manager's Guide with Examples \[2025 ..., accessed August 20, 2025, [https://asana.com/resources/risk-register](https://asana.com/resources/risk-register)